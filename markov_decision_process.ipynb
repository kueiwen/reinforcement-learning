{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b57ed8",
   "metadata": {},
   "source": [
    "# **Markov Decision Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822e086",
   "metadata": {},
   "source": [
    "Markov decision process (MDP) is an imprtant concept regarding reinforcement learning (RL), almost all RL probelms can be modeled as MDP. Not like other ML methods, RL does not need labels or rules, the agent of RL interact with the environment and get reward from the environment, which is the target to optimize. \n",
    "\n",
    "<img src=\"img/reinforcement_learning.png\" width=\"600\">\n",
    "\n",
    "1. environment tells current state\n",
    "2. agent makes an action based on current state\n",
    "3. enviroment return the reward and next state according to the action\n",
    "\n",
    "We can see that here are 3 infromation delivered between agent and environment: state, action and reward. With respect to the action $(a)$, current state $(S_t)$ will switch to next state $(S_{t+1})$. If the environment is fully observable, which means that all $P(S_{t+1}|S_t=s,A=a)$ are known, we saild that the state fulfill Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a95e34",
   "metadata": {},
   "source": [
    "### **Markov Property**\n",
    "\n",
    "The future is only depends on current state. Current state can represent all past.\n",
    "\n",
    "$P(S_{t+1}|S_1,...S_t)=P(S_{t+1}|S_t)$\n",
    "\n",
    "\n",
    "<img src=\"img/markov_process.png\" width=\"600\">\n",
    "\n",
    "This is an example of Markov process, if current state $S_t$ is Standing, than   \n",
    "$P(S_{t+1}=\\text{Sitting}|S_{t}=\\text{Standing})=0.3$  \n",
    "$P(S_{t+1}=\\text{Hands Raised}|S_{t}=\\text{Standing})=0.1$  \n",
    "$P(S_{t+1}=\\text{Foot Forward}|S_{t}=\\text{Standing})=0.5$  \n",
    "$P(S_{t+1}=\\text{Shut Down}|S_{t}=\\text{Standing})=0.1$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f26780",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41e32c47",
   "metadata": {},
   "source": [
    "### **Markov Reward Process**\n",
    "\n",
    "Markov reward process is a tuple $<S, P, R, \\gamma>$\n",
    "\n",
    "$S$: finite set of states  \n",
    "$P$: state transition probability matrix $P_{ss\\prime}=P(S_{t+1}=s\\prime|S_t=s)$  \n",
    "$R$: reward function $R_s=E(R_{t+1}|S_t=s)$  \n",
    "$\\gamma$: discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "<img src=\"img/markov_reward_process.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2554a",
   "metadata": {},
   "source": [
    "Here is an example of Makov Reward Process based on previous Makov process example. Similarly, if current state $S_t$ is Standing, than\n",
    "\n",
    "$R_{S_{t}=\\text{Standing}}=E(R_{t+1}|S_{t}=\\text{Standing})$  \n",
    "$=0.6\\times(-1)+0.1\\times0.1+0.5\\times(-1)+0.1\\times0$  \n",
    "$=-0.6+0.01=0.5=-0.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb877793",
   "metadata": {},
   "source": [
    "#### **Discount Factor**\n",
    "\n",
    "We usually prefer near reward due to uncertainty in the future, so the more future reward will get less value than it's actual reward.\n",
    "\n",
    "$G_t=R_{t+1}+\\gamma R_{t+2}+...=\\sum_{=0}^{\\infty}\\gamma^kR_{t+k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33873b5",
   "metadata": {},
   "source": [
    "#### **Bellman Equation for MRP**\n",
    "\n",
    "Bellman equation is to calculate the state value, which not only contain reward, but discounted value of current state.\n",
    "\n",
    "$v(s)=E(G_t|S_t=s)$\n",
    "    $=E(R_{t+1}+\\gamma R_{t+2}+...|S_t=s)$  \n",
    "    $=E(R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}...)|S_t=s)$  \n",
    "    $=E(R_{t+1}+\\gamma G_{t+1}|S_t=s)$  \n",
    "    $=E(R_{t+1}+\\gamma v{S_{t+1}}|S_t=s)$\n",
    "\n",
    "<img src=\"img/mrp_state_value.png\" width=\"400\">\n",
    "\n",
    "So if $\\gamma=1$, which mean there is no discount,  \n",
    "\n",
    "$v(\\text{Standing})=-1+R_{S_{t}}=-1+(-0.99)=-1.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bb041",
   "metadata": {},
   "source": [
    "### **Markov Decision Process**\n",
    "\n",
    "If add **Action** into Markov Reward Process, then will become Markov Decision Process, which is a tuple $<S,A,P,R,\\gamma>$\n",
    "\n",
    "$S$: finite set of states  \n",
    "$A$: finite set of actions \n",
    "$P$: state transition probability matrix $P_{ss\\prime}^a=P(S_{t+1}=s\\prime|S_t=s, A_t=a)$  \n",
    "$R$: reward function $R_s^a=E(R_{t+1}|S_t=s,A_t=a)$  \n",
    "$\\gamma$: discount factor, $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6a2b7",
   "metadata": {},
   "source": [
    "#### **Policy**\n",
    "\n",
    "A policy is a distribution over actions with a given state.\n",
    "\n",
    "$\\pi(a|s)=P(A_t=a|S_t=s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef25769",
   "metadata": {},
   "source": [
    "#### **Bellman Equation for MDP**\n",
    "\n",
    "State value function: $v_{\\pi}(s)=E_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S+t=s]$\n",
    "\n",
    "Action value function: $q_{\\pi}(s,a)=E_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1},A_{t+1})|S+t=s,A_t=a]$\n",
    "\n",
    "<img src=\"img/mdp_state_value.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa318423",
   "metadata": {},
   "source": [
    "The relation between state and action function:  \n",
    "\n",
    "$v_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)q_{\\pi}(s,a)$\n",
    "\n",
    "$\\implies$ state value is the expected value over action value.\n",
    "\n",
    "$q_{\\pi}(s,a)=R_s^a+\\gamma\\sum_{s\\prime\\in S}P_{ss\\prime}^a v_{\\pi}(s\\prime)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95880e1c",
   "metadata": {},
   "source": [
    "#### **Optimal Value Function**\n",
    "\n",
    "The optimal policy is the policy give max state value and action value.  \n",
    "$v_{*}(s)=max_{\\pi}v_{\\pi}(s)$  \n",
    "$q_{*}(s,a)=max_{\\pi}q_{\\pi}(s,a)$  \n",
    "\n",
    "$\\pi_{*}(a|s)=\\begin{cases} 1\\quad \\text{if}\\ a=\\arg\\max_{a\\in A}q_{*}(s,a) \\\\ 0\\quad \\text{otherwise}\\end{cases}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
