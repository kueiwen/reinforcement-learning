{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97ed2fa",
   "metadata": {},
   "source": [
    "# **Temporal-Difference Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df9caf",
   "metadata": {},
   "source": [
    "Temporal Difference (TD) Learning is similar to the Monte-Carlo method. However, despite it learning from episodes of experience and being model-free, TD has a unique feature. TD will not wait for the episode to end to update the maximum possible reward.  The reward will be updated during the episode based on gained experience.\n",
    "\n",
    "$V(s)\\leftarrow V(S_t)+\\alpha[R_{t+1}+\\gamma(S_{t_1}-V(S_t))]$\n",
    "\n",
    "There are 2 cmmon temporal-differnce learning method: Q-learning and SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d4fc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## **Q-learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e3c9b",
   "metadata": {},
   "source": [
    "Q-learning is a **model-free**, **value-based** and **off-policy** reinforcement learning algorithm to find the best action by updating a table of values, called q-table, which is a data structure of sets of actions and states. We use the Q-learning algorithm to update the values in the table and find the best action for each state, the updating function used by Q-learning is called Q-function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d1507",
   "metadata": {},
   "source": [
    "#### **Q-function**\n",
    "\n",
    "Q-function is the same as action value function in Bellman equation.  \n",
    "\n",
    "$Q_{\\pi}(s,a)=E_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1},A_{t+1})|S+t=s,A_t=a]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc77a85",
   "metadata": {},
   "source": [
    "  \n",
    "In each step, Q-learning update Q-value by,\n",
    "\n",
    "<img src=\"img/q_learning.png\" width=\"200\">  \n",
    "\n",
    "$Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha[R_{t+1}+\\gamma\\max_a Q(S_{t+1},a)-Q(S_t,A_t)]$  \n",
    "\n",
    "where $\\alpha$ is learning rate, \n",
    "\n",
    "$R_{t+1}+\\gamma\\max_a Q(S_{t+1},a)$ is TD target,  \n",
    "\n",
    "$R_{t+1}+\\gamma\\max_a Q(S_{t+1},a)-Q(S_t,A_t)$ is TD error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0cc29a",
   "metadata": {},
   "source": [
    "***Q-learning algorithm***\n",
    "\n",
    "---\n",
    "**Input** MDP $M=<S,s_0,A,P_a(s^{\\prime},s),r(s,a,s^{\\prime}),\\gamma>$  \n",
    "**Output** Q-table\n",
    "\n",
    "Initialize $Q(s,a)$, for all $s\\in S$, $a\\in A(s)$, arbitrarily except that $Q(terminal,.)=0$\n",
    "\n",
    "**repeat**  \n",
    "    $\\quad$ Initialize $s$      \n",
    "    $\\quad$**for each step of episode**        \n",
    "        $\\quad\\quad$**Choose** $a\\in A(s)$ **by Q-table** (e.g., $\\epsilon$-greedy)      \n",
    "            $\\quad\\quad\\quad Q(s,a)\\leftarrow Q(s,a)+\\alpha[r(s,a,s^{\\prime})+\\gamma\\max_{a^{\\prime}\\in A(s^{\\prime})} Q(s^{\\prime},a^{\\prime})-Q(s,a)]$      \n",
    "            $\\quad\\quad\\quad s \\leftarrow s^{\\prime}$     \n",
    "    $\\quad\\quad$**end unitl terminal**       \n",
    "**end** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce894474",
   "metadata": {},
   "source": [
    "## **SARSA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a2195",
   "metadata": {},
   "source": [
    "SARSA is also a **model-free**, **value-based** but **on-policy** reinforcement learning algorithm based on Q-table, the difference between Q-learning and SARSA is the updating equation:\n",
    "\n",
    "<img src=\"img/sarsa.png\" width=\"100\">  \n",
    "\n",
    "$Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha[R_{t+1}+\\gamma Q(S_{t+1},A^{\\prime})-Q(S_t,A_t)]$      \n",
    "\n",
    "$A^{\\prime}$ is the action chosen from $S_{t+1}$ based on Q-table. \n",
    "\n",
    "$R_{t+1}+\\gamma\\max_a Q(S_{t+1},a)$ is TD target,  \n",
    "\n",
    "$R_{t+1}+\\gamma\\max_a Q(S_{t+1},a)-Q(S_t,A_t)$ is TD error.\n",
    "\n",
    "\n",
    "Not like Q-learning to choose next action by updated Q-value, SARSA is based on Q-table to choose the next action to update Q-table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424f043",
   "metadata": {},
   "source": [
    "***SARSA algorithm***\n",
    "\n",
    "---\n",
    "**Input** MDP $M=<S,s_0,A,P_a(s^{\\prime},s),r(s,a,s^{\\prime}),\\gamma>$  \n",
    "**Output** Q-table\n",
    "\n",
    "\n",
    "Initialize $Q(s,a)$, for all $s\\in S$, $a\\in A(s)$, arbitrarily except that $Q(terminal,.)=0$\n",
    "\n",
    "**repeat**  \n",
    "    $\\quad$ Initialize $s, a$      \n",
    "    $\\quad$**for each step of episode**        \n",
    "        $\\quad\\quad$**Choose** $a^{\\prime}\\in A(s)$ **by Q-table** (e.g., $\\epsilon$-greedy)      \n",
    "            $\\quad\\quad\\quad Q(s,a)\\leftarrow Q(s,a)+\\alpha[r(s,a,s^{\\prime})+\\gamma Q(s^{\\prime},a^{\\prime})-Q(s,a)]$      \n",
    "            $\\quad\\quad\\quad s \\leftarrow s^{\\prime}$     \n",
    "            $\\quad\\quad\\quad a \\leftarrow a^{\\prime}$     \n",
    "    $\\quad\\quad$**end unitl terminal**       \n",
    "**end** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3644d7",
   "metadata": {},
   "source": [
    "In this 4X4 grid, we need to go to the end point with less steps, the upper left corner case and the lower right corner case are the end points. \n",
    "\n",
    "<img src=\"img/policy_iteration_example.png\" width=\"400\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3321175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(state, action):\n",
    "    if state in [0, 15]:\n",
    "        return state\n",
    "    if action == 0: # up\n",
    "        if state in [1, 2, 3]:\n",
    "            return state\n",
    "        else:\n",
    "            return state - 4\n",
    "    if action == 1: # down\n",
    "        if state in [12, 13, 14]:\n",
    "            return state\n",
    "        else:\n",
    "            return state + 4\n",
    "    if action == 2: # left\n",
    "        if state in [4, 8, 12]:\n",
    "            return state\n",
    "        else:\n",
    "            return state - 1\n",
    "    if action == 3: # right\n",
    "        if state in [3, 7, 11]:\n",
    "            return state\n",
    "        else:\n",
    "            return state + 1\n",
    "\n",
    "def get_action(state, q, epsilon):\n",
    "    # epsilon greedy policy\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.choice([0, 1, 2, 3])\n",
    "    else:\n",
    "        # greedy policy\n",
    "        action = np.argmax(q[state])\n",
    "        # if there are multiple actions with the same max value, choose one randomly\n",
    "        if list(q[state]).count(q[state][action]) > 1:\n",
    "            action = np.random.choice(np.where(q[state] == q[state][action])[0])\n",
    "    return action\n",
    "\n",
    "\n",
    "def update_q(q, state, action, reward, next_state, alpha, gamma, method='q-learning'):\n",
    "    # Q-learning update rule\n",
    "    if method == 'q-learning':\n",
    "        q[state, action] += alpha * (reward + gamma * np.max(q[next_state]) - q[state, action])\n",
    "    elif method == 'sarsa':\n",
    "        action_next = get_action(next_state, q, 0)\n",
    "        q[state, action] += alpha * (reward + gamma * q[next_state, action_next] - q[state, action])\n",
    "    return q\n",
    "\n",
    "def q_learning(num_episodes=10000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    # Initialize variables\n",
    "    q = np.zeros((16, 4))  # Q-table with 16 states and 4 actions\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = np.random.randint(1, 16)  # Random initial state\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = get_action(state, q, epsilon)\n",
    "            next_state = transition(state, action)\n",
    "\n",
    "            if next_state in (0,15):  # Goal state\n",
    "                reward = 1\n",
    "                done = True\n",
    "            else:\n",
    "                reward = -0.01\n",
    "\n",
    "            q = update_q(q, state, action, reward, next_state, alpha, gamma)\n",
    "            state = next_state\n",
    "\n",
    "    return q\n",
    "\n",
    "\n",
    "\n",
    "def sarsa(num_episodes=10000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    # Initialize variables\n",
    "    q = np.zeros((16, 4))  # Q-table with 16 states and 4 actions\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = np.random.randint(1, 16)  # Random initial state\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = get_action(state, q, epsilon)\n",
    "            next_state = transition(state, action)\n",
    "\n",
    "            if next_state in (0,15):  # Goal state\n",
    "                reward = 1\n",
    "                done = True\n",
    "            else:\n",
    "                reward = -0.01\n",
    "\n",
    "            q = update_q(q, state, action, reward, next_state, alpha, gamma, )\n",
    "            state = next_state\n",
    "\n",
    "    return q\n",
    "\n",
    "def mapping_policy(pi):\n",
    "    policy = dict()\n",
    "    for k in range(1, 15):\n",
    "        idx = np.where(pi[k] == np.max(pi[k]))[0]\n",
    "        policy[k] = []\n",
    "        if 0 in idx:\n",
    "            policy[k].append('up')\n",
    "        if 1 in idx:\n",
    "            policy[k].append('down')\n",
    "        if 2 in idx:\n",
    "            policy[k].append('left')\n",
    "        if 3 in idx:\n",
    "            policy[k].append('right')\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023aa029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Q-learning---\n",
      "State 1: ['down']\n",
      "State 2: ['down']\n",
      "State 3: ['down']\n",
      "State 4: ['right']\n",
      "State 5: ['right']\n",
      "State 6: ['right']\n",
      "State 7: ['down']\n",
      "State 8: ['down']\n",
      "State 9: ['right']\n",
      "State 10: ['down']\n",
      "State 11: ['down']\n",
      "State 12: ['right']\n",
      "State 13: ['right']\n",
      "State 14: ['right']\n",
      "---SARSA---\n",
      "State 1: ['down']\n",
      "State 2: ['down']\n",
      "State 3: ['down']\n",
      "State 4: ['right']\n",
      "State 5: ['right']\n",
      "State 6: ['right']\n",
      "State 7: ['down']\n",
      "State 8: ['right']\n",
      "State 9: ['right']\n",
      "State 10: ['down']\n",
      "State 11: ['down']\n",
      "State 12: ['right']\n",
      "State 13: ['right']\n",
      "State 14: ['right']\n"
     ]
    }
   ],
   "source": [
    "print('---Q-learning---')\n",
    "q = q_learning()\n",
    "policy = mapping_policy(q)\n",
    "for k, v in policy.items():\n",
    "    print(f\"State {k}: {v}\")\n",
    "\n",
    "print('---SARSA---')\n",
    "q = sarsa()\n",
    "policy = mapping_policy(q)\n",
    "for k, v in policy.items():\n",
    "    print(f\"State {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bf42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
