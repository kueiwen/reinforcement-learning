{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d669c2",
   "metadata": {},
   "source": [
    "# **Trust Region Policy Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd361bcc",
   "metadata": {},
   "source": [
    "Policy gradient is based on gradient ascent, which is to update parameters by the first-order derivation. However, if the surface has a high curvature, we will make a relative large update to parameters, that is not stable.\n",
    "\n",
    "TRPO limits the changes on the parameter within one iteration to stablize the policy update process, and make sure that the update is toward a better policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7bdce8",
   "metadata": {},
   "source": [
    "## **Minorize-Maximization Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4fc1e",
   "metadata": {},
   "source": [
    "The Minorize-Maximization (MM) algorithm gives us the theoretical guarantees that the updates always result in improving the expected rewards. A simple one line explanation of this algorithm is that it iteratively maximizes a simpler lower bound function (lower bound with respect to the actual reward function), approximating the reward function locally.\n",
    "\n",
    "<img src=\"img/mm_algorithm.png\" width=\"800\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f76236",
   "metadata": {},
   "source": [
    "The discounted reward can be expressed as,\n",
    "\n",
    "$$\\eta(\\pi)=E_{\\tau\\sim\\pi}[R(\\tau)]=E_{\\tau\\sim\\pi_{\\theta}}[\\sum_{t=1}^{\\infty}\\gamma^{t}r(s_t)]$$\n",
    "\n",
    "As metioned in [reinforcement_learning_concept](https://github.com/kueiwen/reinforcement-learning/blob/main/reinforcement_learning_concept.ipynb), there are two function from Bellman equation: value function $V_{\\pi}$ and action value function $Q_{\\pi}$, here we introduce another function named advantage function $A_{\\pi}$:\n",
    "\n",
    "$$A_{\\pi}(s,a)=Q_{\\pi}(s,a)-V_{\\pi}(s)$$\n",
    "\n",
    "Advantage function calcuated the addtional action value if current state is $s$.\n",
    "\n",
    "Then we can use advantage function to get how another policy $\\tilde{\\pi}$ is better than current policy $\\pi$\n",
    "\n",
    "$$\\eta(\\tilde{\\pi})=\\eta(\\pi)+E_{\\tau\\sim\\tilde{\\pi}}[\\sum_{t=1}^{\\infty}\\gamma^{t}A_{\\tilde{\\pi}}(s_t,a_t)]=\\eta(\\pi)+\\sum_{s}\\rho_{\\tilde{\\pi}}(s)\\sum_{a}\\tilde{\\pi}(a|s)A_{\\pi}(s,a)$$\n",
    "\n",
    "\n",
    "where $\\rho_{\\pi}(s)$ is the probability distribution of states under policy $\\pi$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095170b6",
   "metadata": {},
   "source": [
    "This equation implies that any policy update $\\pi\\to\\tilde{\\pi}$ that has a non-negative expected advantage at *every* state $s$, which mean that $\\sum_{a}\\tilde{\\pi}(a|s)A_{\\pi}(s,a)\\geq0$ is guaranteed to increase the policy performance $\\eta$ or keep the same.\n",
    "\n",
    "However, it is hard to to use deterministic policy $\\tilde{\\pi}(s)=\\text{arg}\\max_{a}A(s,a)$ for all states to get a better policy that at least one state with positive advantage and others are zero, which is due to the difference between $\\rho_{\\tilde{\\pi}}$ and $\\rho_{\\pi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca486dc",
   "metadata": {},
   "source": [
    "Therefore, here comes anther equation for local approximation of $\\eta$,\n",
    "\n",
    "$$L_{\\pi}(\\tilde{\\pi})=\\eta(\\pi)+\\sum_{s}\\rho_{\\tilde{\\pi}}(s)\\sum_{a}\\tilde{\\pi}(a|s)A_{\\pi}(s,a)$$\n",
    "\n",
    "The $L_{\\pi}$ use the state probability disrtibution under $\\pi$ rather than $\\tilde{\\pi}$.\n",
    "\n",
    "And for any policy parameter $\\theta_{0}$,\n",
    "\n",
    "$$L_{\\pi_{\\theta_{0}}}(\\pi_{\\theta})=\\eta(\\pi_{\\theta_{0}})$$\n",
    "\n",
    "$$\\nabla_{\\theta}L_{\\pi_{\\theta_{0}}}(\\pi_{\\theta})|_{\\theta=\\theta_{0}}=\\nabla_{\\theta}\\eta(\\pi_{\\theta})|_{\\theta=\\theta_{0}}$$\n",
    "\n",
    "which implies that a small step $\\pi_{\\theta_0}\\to\\tilde{\\pi}$ also contributes to $\\eta$, but does not give us any guidance on how big of a step to take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1779c",
   "metadata": {},
   "source": [
    "An policy updating schema is introduced to overcome this issue, called conservative policy iteration, which can provide explicit lower bounds on the improvement of $\\eta$.\n",
    "\n",
    "$$\\pi_{\\text{new}}(a|s)=(1-\\alpha)\\pi_{\\text{old}}(a|s)+\\alpha\\pi^{\\prime}(a|s)$$\n",
    "\n",
    "where $\\pi_{\\text{old}}$ is current policy, and let $\\pi^{\\prime}=\\arg\\max_{\\pi^{\\prime}}L_{\\text{old}}(\\pi^{\\prime})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094abb39",
   "metadata": {},
   "source": [
    "And the lower bounds of improvement,\n",
    "\n",
    "$$\\eta(\\pi_{\\text{new}})\\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})-\\frac{2\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2$$\n",
    "\n",
    "$$\\text{where }\\epsilon=\\max_{s}|E_{a\\sim\\pi^{\\prime}(a|s)}[A_{\\pi}(s,a)]|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621c40d",
   "metadata": {},
   "source": [
    "We can make this more applicable to pratical problem by replacing $\\alpha$ with the distance between $\\pi$ and $\\tilde{\\pi}$, we use total variance divergence $D_{\\text{VT}}(p||q)=\\frac{1}{2}\\sum_{i}|p_i-q_i|$ for discrete probability distribution $p$ and $q$. So the distance between $\\pi$ and $\\tilde{\\pi}$ will be,\n",
    "\n",
    "$$D_{\\text{VT}}^{\\max}(\\pi,\\tilde{\\pi})=\\max_{a}D_{\\text{VT}}(\\pi(.|s)||\\tilde{\\pi}(.|s))$$\n",
    "\n",
    "So if $\\alpha=D_{\\text{VT}}^{\\max}(\\pi,\\tilde{\\pi})$\n",
    "\n",
    "\n",
    "$$\\eta(\\pi_{\\text{new}})\\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})-\\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2$$\n",
    "\n",
    "$$\\text{where }\\epsilon=\\max_{s,a}|A_{\\pi}(s,a)|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18882788",
   "metadata": {},
   "source": [
    "Because $D_{\\text{TV}}(p||q)^2\\leq D_{\\text{KL}}(p||q)$, the bound can be,\n",
    "\n",
    "$$\\eta(\\pi_{\\text{new}})\\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}})-CD_{\\text{KL}}^{\\max}(\\pi,\\tilde{\\pi})$$\n",
    "\n",
    "$$\\text{where }C=\\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2$$\n",
    "\n",
    "KL here represent KL divergence, which is to calculate the similarity of two distribution,  \n",
    "\n",
    "\n",
    "$$D_{\\text{KL}}(p,q)=\\sum_{i}p_i\\log(\\frac{p_i}{q_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720799bb",
   "metadata": {},
   "source": [
    "By conservative policy iteration, we can get a sequence of monotonically improved policies $\\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\eta(\\pi_2)\\leq...$.   \n",
    "\n",
    "Based on Minorize-Maximization (MM) algorithm, let surrogate function $M_i(\\pi)=L_{\\pi_{i}}(\\pi)-CD_{\\text{KL}}^{\\max}(\\pi_i,\\pi)$, then,\n",
    "\n",
    "\n",
    "$$\\eta(\\pi_{i+1})\\geq M_i(\\pi_{i+1})$$\n",
    "\n",
    "$$\\eta(\\pi_{i})=M_i(\\pi_{i})$$\n",
    "\n",
    "$$\\text{therefore  }\\eta(\\pi_{i+1})-\\eta(\\pi_{i})\\geq M_i(\\pi_{i+1})-M_i(\\pi_{i})$$\n",
    "\n",
    "By maximizing $M_i$ at each iteration, we can gaurantee that $\\eta$ is non-decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5930056",
   "metadata": {},
   "source": [
    "***policy iteration algorithm guaranteeing non-decreasing expected return $\\eta$***\n",
    "\n",
    "---\n",
    "Initialize $\\pi_{0}$\n",
    "\n",
    "**repeat**  \n",
    "    $\\quad$ Computing all advantage values $A_{\\pi_{i}}(s,a)$      \n",
    "    $\\quad$ Solve the constrained optimization problem        \n",
    "    $\\quad\\quad \\pi_{i+1}=\\arg\\max_{\\pi}[L_{\\pi_{i}}(\\pi)-CD_{\\text{KL}}^{\\max}(\\pi_i,\\pi)]$      \n",
    "            $\\quad\\quad$ where $C=4\\epsilon\\gamma/(1-\\gamma)^2$       \n",
    "            $\\quad\\quad\\quad$ and $L_{\\pi_{i}}(\\pi)=\\eta(\\pi_{i})+\\sum_{s}\\rho_{\\pi{i}}(s)\\sum_a \\pi(a|s)A_{\\pi_{i}}(s,a)$     \n",
    "**until convergence** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f6f6e",
   "metadata": {},
   "source": [
    "## **Optimization policy parameter**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c256d6",
   "metadata": {},
   "source": [
    "From above derivation, we know that TRPO use a constraint on KL divergence to get a small step update, which is relatively robust.\n",
    "\n",
    "To simplify the function, we use parameter $\\theta$ to represent the policy $\\pi_{\\theta}$\n",
    "\n",
    "$$\\max_{\\theta}[L_{\\theta_{\\text{old}}}(\\theta)-CD_{\\text{KL}}^{\\max}(\\theta_{\\text{old}},\\theta)]$$\n",
    "\n",
    "However, if we use penalty coefficient $C$ the step size will be very small, so we cahnge to use a constraint on the KL divergence between the new policy and old policy,\n",
    "\n",
    "\n",
    "$$\\max_{\\theta}L_{\\theta_{\\text{old}}}(\\theta)$$\n",
    "\n",
    "$$\\text{subject to  }\\overline{D}_{\\text{KL}}^{\\rho_{\\theta_{\\text{old}}}}(\\theta_{\\text{old}},\\theta)\\leq\\delta$$\n",
    "\n",
    "where $\\overline{D}_{\\text{KL}}^{\\rho}(\\theta_{1},\\theta_{2}):=E_{s\\sim\\rho}D_{\\text{KL}}(\\pi_{\\theta_{1}}(.|s),\\pi_{\\theta_{2}}(.|s))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e2b193",
   "metadata": {},
   "source": [
    "Maximizing $L_{\\theta_{\\text{old}}}(\\theta)$ can be considered as maximizing $\\sum_{s}\\rho_{\\theta_{\\text{old}}}(s)\\sum_{a}\\pi_{\\theta}(a|s)A_{\\theta_{\\text{old}}}(s,a)$. If we replace advantage function $A$ as $Q$, and use $q$ denote the sample distribution,\n",
    "\n",
    "$$\\max_{\\theta}E_{s\\sim\\rho_{\\theta_{\\text{old}}},a\\sim q}[\\frac{\\pi_{\\theta}(a|s)}{q(a|s)}Q_{\\theta_{\\text{old}}}(s,a)]$$\n",
    "\n",
    "$$\\text{subject to  }E_{s\\sim\\rho_{\\theta_{\\text{old}}}}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(.|s)||\\pi_{\\theta}(.|s))]\\leq\\delta$$\n",
    "\n",
    "There are two sampling schemes to estimate Q value: **single path** and **vine**.\n",
    "<img src=\"img/trpo_path.png\" width=\"600\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587f1a4",
   "metadata": {},
   "source": [
    "##### **Single Path**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f10ade",
   "metadata": {},
   "source": [
    "Single path is typically used in policy gradient, and is based on sampling one trajectory.\n",
    "\n",
    "1. Collect a sequence of statesby smpling $s_0\\sim\\rho_0$.\n",
    "\n",
    "2. Simulate the policy $\\pi_{\\theta_{\\text{old}}}$ for some number of timesteps to gernerate a trajectory $s_0,a_0,s_1,a_1,...,s_{T-1},a_{T-1},s_T$.\n",
    "\n",
    "3. Compute $Q_{\\theta_{\\text{old}}}$ at each state-action pair $(s_t,a_t)$ by taking discounted sum of future rewards along the trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a1f41",
   "metadata": {},
   "source": [
    "##### **Vine**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9d804",
   "metadata": {},
   "source": [
    "Vine involves constructing a rollout set and then performing multiple actions from each state in the rollout set.\n",
    "\n",
    "1. Collect a sequence of statesby smpling $s_0\\sim\\rho_0$.\n",
    "\n",
    "2. Simulate the policy $\\pi_{\\theta}$ to gernerate a number of trajectories.\n",
    "\n",
    "3. Choose a subset of $N$ states along the trajectories, called rollout set. ($s_1,s_2,...,s_{N}$)\n",
    "\n",
    "4. For each state $s_n$ in rollout set, sample $K$ actions according to $a_{n,k}\\sim q(.|s_n)$.\n",
    "\n",
    "5. For each action $a_{n,k}$ sampled at each state $s_n$, estimate $\\hat{Q}_{\\theta_{i}}(s_n,a_{n,k})$ by performing a rollout starting with state $s_n$ and acttion $a_{n,k}$.\n",
    "    * By using the same random number sequence for the noise in each of the $K$ rollouts, i.e., *common random numbers*, the variance of the Q-value differences between rollouts can be largely reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b323c",
   "metadata": {},
   "source": [
    "In finite action spaces, we can generate a rollout for every possible action froma given state. The contribution to $L_{\\theta_{\\text{old}}}$ from a single state $s_n$ is,\n",
    "\n",
    "$$L_n(\\theta)=\\sum_{k=1}^{K}\\pi_{\\theta}(a_k|s_n)\\hat{Q}(s_n,a_{n,k})$$\n",
    "\n",
    "In large or continuous action spaces, we can construct an estimator of the surrogate objective using importance sampling. The self-normalized estimator of $L_{\\theta_{\\text{old}}}$ obtained at a single state $s_n$ is,\n",
    "\n",
    "$$L_n(\\theta)=\\frac{\\sum_{k=1}^{K}\\frac{\\pi_{\\theta}(a_{n,k}|s_n)}{\\pi_{\\theta_{\\text{old}}}(a_{n,k}|s_n)}\\hat{Q}(s_n,a_{n,k})}{\\sum_{k=1}^{K}\\frac{\\pi_{\\theta}(a_{n,k}|s_n)}{\\pi_{\\theta_{\\text{old}}}(a_{n,k}|s_n)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201dc8fb",
   "metadata": {},
   "source": [
    "To compare the two schemes, vine gives much better estimation of advantage value, but need to generate multiple trjectories from each state in the rollout set, which limits this algorithm to setting where the system can be reset to an arbitrary state. The single path, instead, does not need state resets and can be directly implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a63ea1",
   "metadata": {},
   "source": [
    "## **TRPO Algorithm Steps**\n",
    "\n",
    "1. Use the single path or vine procedures to collect a set of state-action pairs along wirh Monte Carlos estimates of the Q-values.\n",
    "\n",
    "2. By averaging over sampling, construct the estimated objective ans constraint.\n",
    "\n",
    "$$\\max_{\\theta}L_{\\theta_{\\text{old}}}(\\theta)$$\n",
    "\n",
    "$$\\text{subject to  }\\overline{D}_{\\text{KL}}^{\\rho_{\\theta_{\\text{old}}}}(\\theta_{\\text{old}},\\theta)\\leq\\delta$$\n",
    "\n",
    "\n",
    "3. Solve this optimization problem by conjugate gradient algorithm followed by a line search to update the policy's parameter vector $\\theta$.\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f1ac70",
   "metadata": {},
   "source": [
    "#### **Conjugate Gradient**\n",
    "\n",
    "Conjugate gradient is used to solved linear equation or to optimize quadratic function.\n",
    "\n",
    "The following linear equation and quadratirc optimization are equvalent.\n",
    "\n",
    "Linear equation\n",
    "$$Ax=b$$\n",
    "\n",
    "quadratirc optimization\n",
    "$$\\max_x \\frac{1}{2}x^{T}Ax-b^Tx$$\n",
    "$$\\text{subject to }Ax-b=0$$\n",
    "\n",
    "Conjugate gradient is much more efficient than gradient ascent, because conjugate gradient method is a line search method but for every move, it would not undo part of the moves done previously.\n",
    "\n",
    "<img src=\"img/conjugate_gradient.png\" width=\"600\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd921232",
   "metadata": {},
   "source": [
    "***conjugate gradient***\n",
    "\n",
    "---\n",
    "$r_0:=b-Ax$   \n",
    "$p_0:=r_0$   \n",
    "$k:=0$   \n",
    "\n",
    "**repeat**  \n",
    "    $\\quad \\alpha_k:=\\frac{r_k^Tr_k}{p_k^TAp_k}\\to$ how far to move in direction $p$          \n",
    "    $\\quad x_{k+1}:=x_k+\\alpha_kp_k\\to$ the next point     \n",
    "    $\\quad r_{k+1}:=r_k-\\alpha_kAp_k\\to$ remaining error from the optimal point      \n",
    "    $\\quad$ if $r_{k+1}$ is suffucuently small, then exit loop    \n",
    "    $\\quad \\beta_k:=\\frac{r_{k+1}^Tr_{k+1}}{r_k^tr_k}\\to$ the new direction, A-orthogonal    \n",
    "    $\\quad p_{k+1}:=r_{k+1}+\\beta_{k}p_{k}\\to$ nex direction to go   \n",
    "    $\\quad k:=k+1$    \n",
    "**end** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4d4c6",
   "metadata": {},
   "source": [
    "For TRPO, the function of $x$ here, which is $A$ in above pseudo code, is fisher vector product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d89381",
   "metadata": {},
   "source": [
    "#### **line search**\n",
    "\n",
    "Since he theoritical TRPO optimization is not easy to achieve, TRPO make some approximations to get the result. With Taylor expand e objective and constraint o lead order around $\\theta$,\n",
    "\n",
    "$$L_{\\theta_{\\text{old}}}(\\theta)\\approx g^T(\\theta-\\theta_{\\text{old}})$$\n",
    "\n",
    "$$\\overline{D}_{\\text{KL}}^{\\rho_{\\theta_{\\text{old}}}}(\\theta_{\\text{old}},\\theta)\\sim\\frac{1}{2}(\\theta-\\theta_{\\text{old}})^2H(\\theta-\\theta_{\\text{old}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135253d",
   "metadata": {},
   "source": [
    "Here we get an aproximate optimization problem,\n",
    "\n",
    "$$\\max_\\theta g^T(\\theta-\\theta_{\\text{old}})$$\n",
    "$$\\text{subject to  }\\frac{1}{2}(\\theta-\\theta_{\\text{old}})^2H(\\theta-\\theta_{\\text{old}})\\leq\\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefb94a",
   "metadata": {},
   "source": [
    "Based on Lagrangian duality, we can update parameter $\\theta$ by,\n",
    "\n",
    "$$\\theta_{k+1}\\leftarrow\\theta_k+\\alpha^j\\sqrt{\\frac{2\\delta}{g^{T}H^{-1}g}}H^{-1}g$$\n",
    "\n",
    "where $\\alpha\\in(0,1)$ is backtracking coefficient, and $j$ is the smallest non-negative integer such that $\\pi_{\\theta_{k+1}}$ satisfied the KL constraint and produces a positive surrogate advantage. If no $\\alpha^j$, the algorithm is to calculate Natural Policy Gradient. However, due to the error from Tylor expansion, this may not satisfy the KL constraint, or actually improve the surrogate advange, so the $\\alpha^j$ is a modification for backtracking line search rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34295467",
   "metadata": {},
   "source": [
    "***line search for TRPO***\n",
    "\n",
    "---\n",
    "Compute proposed policy step $\\Delta_k=\\sqrt{\\frac{2\\delta}{\\hat{g}_{k}^{T}\\hat{H}^{-1}\\hat{g}_{k}}}\\hat{H}^{-1}\\hat{g}_{k}$\n",
    "\n",
    "**repeat for k in 0,1,...,L**  \n",
    "    $\\quad$ Compute proposed update $\\theta=\\theta_k+\\alpha^j\\Delta_k$       \n",
    "    $\\quad$ **if** $L_{\\theta_k}\\geq 0$ and $\\overline{D}_{KL}(\\theta,\\theta_k)\\leq\\delta$        \n",
    "    $\\quad\\quad$ accept the update and set $\\theta_{k+1}=\\theta_k+\\alpha^j\\Delta_k$   \n",
    "    $\\quad\\quad$ break      \n",
    "**end** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c9ceb",
   "metadata": {},
   "source": [
    "***TRPO***\n",
    "\n",
    "---\n",
    "**Input:** initial policy parameters $\\theta_0$, value function parameter $\\phi_0$     \n",
    "**Hyperparameters:** KL-divergence limit $\\delta$, backtracking coefficient $\\alpha$, maximum numbe rof backtrackign steps $K$\n",
    "\n",
    "**repeat for k in 0,1,...**  \n",
    "    $\\quad$ Collect set to trajectories $\\cal{D}_k$ on policy $\\pi_k=\\pi(\\theta_k)$       \n",
    "    $\\quad$ Compute rewards-to-go $\\hat{R}_t$     \n",
    "    $\\quad$ Estimate advantages $\\hat{A}_t^{\\pi_k}$ using any advantage estimation algorithm based on current value function $V_{\\phi_k}$       \n",
    "    $\\quad$ Estimate policy gradient $\\hat{g}_k=\\frac{1}{|\\cal{D}_k|}\\sum_{r\\in\\cal{D}_k}\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)|_{\\theta_{k}}\\hat{A}_t$        \n",
    "    $\\quad$ Use conjugate gradient with $n_{cg}$ iterations to obtain $x_k\\approx\\hat{H}_{k}^{-1}\\hat{g}_{k}$      \n",
    "    $\\quad$ Estimate proposed step $\\Delta_k\\sim\\sqrt{\\frac{2\\delta}{x_{k}^{T}\\hat{H}_{k}x_{k}}}x_{k}$     \n",
    "    $\\quad$ Perform backtracking line search with exponential decay to obtain final update $\\theta_{k+1}\\leftarrow\\theta_{k}+\\alpha^{j}\\Delta_{k}$    \n",
    "    $\\quad$ Fit value function by regression on mean-squared error, $\\phi_{k+1}=\\arg\\max_{\\phi}\\frac{1}{|\\cal{D}_k|T}\\sum_{r\\in\\cal{D}_k}\\sum_{t=0}^{T}[V_{\\phi}(s_t)-\\hat{R}_t]^2$       \n",
    "**end** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fca1cb4",
   "metadata": {},
   "source": [
    "#### Policy and value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f73cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.optimize # For L-BFGS\n",
    "import numpy as np\n",
    "from typing import Tuple, Callable, Dict, Any\n",
    "\n",
    "# Precompute constant\n",
    "LOG_2_PI = np.log(2 * np.pi)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    A Gaussian policy network for continuous action spaces.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Dimension of the state space.\n",
    "        hidden_dim: Dimension of the hidden layers.\n",
    "        output_dim: Dimension of the action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super(Policy, self).__init__()\n",
    "        self.inputLayer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hiddenLayer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.outputLayer.weight.data.uniform_(-0.003, 0.003)\n",
    "        self.outputLayer.bias.data.uniform_(-0.003, 0.003)\n",
    "\n",
    "        # Learnable log standard deviation by nn.Parameter\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, output_dim))\n",
    "        # Clamping log_std can improve stability\n",
    "        self.log_std_min = -20\n",
    "        self.log_std_max = 2\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass to get action distribution parameters.\n",
    "\n",
    "        Args:\n",
    "            state: Input state tensor.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - action_mean: Mean of the action distribution.\n",
    "            - action_log_std: Log standard deviation of the action distribution.\n",
    "            - action_std: Standard deviation of the action distribution.\n",
    "        \"\"\"\n",
    "        x = torch.tanh(self.inputLayer(x))\n",
    "        x = torch.tanh(self.hiddenLayer(x))\n",
    "        action_mean = self.outputLayer(x)\n",
    "\n",
    "        # Clamp log_std for stability\n",
    "        self.log_std.data.clamp_(self.log_std_min, self.log_std_max)\n",
    "\n",
    "        action_log_std = self.log_std.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_log_std)\n",
    "        return action_mean, action_log_std, action_std\n",
    "    \n",
    "    def get_log_probability_density(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the log probability density of actions under the policy.\n",
    "\n",
    "        Args:\n",
    "            states: State tensor.\n",
    "            actions: Action tensor.\n",
    "\n",
    "        Returns:\n",
    "            Log probability density for each state-action pair.\n",
    "        \"\"\"\n",
    "        action_mean, action_log_std, action_std = self.forward(states)\n",
    "        var = torch.exp(action_log_std).pow(2)\n",
    "        log_prob_per_dim = -0.5 * (((actions - action_mean) / action_std)**2) \\\n",
    "                           - action_log_std \\\n",
    "                           - 0.5 * LOG_2_PI\n",
    "        return log_prob_per_dim.sum(dim=1, keepdim=True)\n",
    "    \n",
    "    def get_KL_divergence(self, states: torch.Tensor, actions: torch.Tensor, old_log_prob: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Estimate the KL divergence D_KL(old_policy || current_policy) using samples.\n",
    "        Assumes 'old_log_prob' contains log probabilities from the sampling policy.\n",
    "\n",
    "        Args:\n",
    "            states: State tensor.\n",
    "            actions: Action tensor (sampled from the old policy).\n",
    "            old_log_prob: Log probability of the actions under the old policy.\n",
    "\n",
    "        Returns:\n",
    "            Mean KL divergence estimate.\n",
    "        \"\"\"\n",
    "        current_log_prob = self.get_log_probability_density(states, actions)\n",
    "        kl_div = old_log_prob - current_log_prob\n",
    "        return kl_div.mean()\n",
    "    \n",
    "    def get_action(self, state: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample or get the mean action from the policy.\n",
    "\n",
    "        Args:\n",
    "            state: Input state tensor (should be preprocessed, e.g., unsqueezed).\n",
    "            deterministic: If True, return the mean action. Otherwise, sample.\n",
    "\n",
    "        Returns:\n",
    "            Action tensor.\n",
    "        \"\"\"\n",
    "        with torch.no_grad(): # No need to track gradients for action selection\n",
    "            action_mean, _, action_std = self.forward(state)\n",
    "            if deterministic:\n",
    "                return action_mean\n",
    "            else:\n",
    "                normal = torch.distributions.normal.Normal(action_mean, action_std)\n",
    "                return normal.sample()\n",
    "    \n",
    "\n",
    "class Value(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP value function network.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Dimension of the state space.\n",
    "        hidden_dim: Dimension of the hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super(Value, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.value_head.weight.data.uniform_(-0.003, 0.003)\n",
    "        self.value_head.bias.data.uniform_(-0.003, 0.003)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(x).unsqueeze(0)\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        value = self.value_head(x)\n",
    "        return value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb5a84",
   "metadata": {},
   "source": [
    "#### Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f6a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_log_density(actions: torch.Tensor, means: torch.Tensor, log_stds: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Calculates log probability density for a Gaussian distribution.\"\"\"\n",
    "    # Assuming policy_net.get_log_probability_density implements this correctly\n",
    "    # This is just a placeholder signature\n",
    "    # Example implementation (matches previous Policy review):\n",
    "    stds = torch.exp(log_stds)\n",
    "    log_prob_per_dim = -0.5 * (((actions - means) / stds)**2) \\\n",
    "                       - log_stds \\\n",
    "                       - 0.5 * np.log(2 * np.pi)\n",
    "    return log_prob_per_dim.sum(dim=1, keepdim=True)\n",
    "\n",
    "def set_flat_params_to(model: nn.Module, flat_params: torch.Tensor):\n",
    "    \"\"\"Sets model parameters from a flat tensor.\"\"\"\n",
    "    offset = 0\n",
    "    for param in model.parameters():\n",
    "        numel = param.numel()\n",
    "        # Slice the flat_params and reshape it to the correct parameter shape\n",
    "        param.data.copy_(flat_params[offset:offset + numel].view_as(param.data))\n",
    "        offset += numel\n",
    "\n",
    "def get_flat_params_from(model: nn.Module) -> torch.Tensor:\n",
    "    \"\"\"Flattens model parameters into a single tensor.\"\"\"\n",
    "    return torch.cat([p.detach().view(-1) for p in model.parameters()])\n",
    "\n",
    "def get_flat_grad_from(model, grad_grad=False):\n",
    "    '''Get first or second grad of param of model'''\n",
    "    grad = []\n",
    "    for param in model.parameters():\n",
    "        if grad_grad:\n",
    "            grad.append(param.grad.grad.view(-1))\n",
    "        else:\n",
    "            grad.append(param.grad.view(-1))\n",
    "    return torch.cat(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e23ded",
   "metadata": {},
   "source": [
    "#### TRPO function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9454912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(fvp_callable: Callable, b: torch.Tensor, n_steps: int, residual_tol: float = 1e-10) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Solves the linear system A*x = b using the conjugate gradient method,\n",
    "    where A is implicitly defined by the Fisher-vector product function fvp_callable.\n",
    "\n",
    "    Args:\n",
    "        fvp_callable: A function that takes a vector v and returns A*v (the FVP).\n",
    "        b: The right-hand side vector of the system A*x = b.\n",
    "        nsteps: Maximum number of iterations.\n",
    "        residual_tol: Tolerance for convergence.\n",
    "\n",
    "    Returns:\n",
    "        The solution vector x.\n",
    "    \"\"\"\n",
    "    x = torch.zeros_like(b)\n",
    "    r = b.clone() # Initial residual: r = b - A*x = b (since x=0)\n",
    "    p = r.clone() # Initial search direction\n",
    "    rdotr = torch.dot(r, r)\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        Ap = fvp_callable(p) # Calculate A*p (Fisher-vector product)\n",
    "        alpha = rdotr / (torch.dot(p, Ap) + 1e-8) # Add epsilon for stability\n",
    "        x += alpha * p\n",
    "        r -= alpha * Ap\n",
    "        new_rdotr = torch.dot(r, r)\n",
    "        if new_rdotr < residual_tol:\n",
    "            break\n",
    "        beta = new_rdotr / (rdotr + 1e-8) # Add epsilon for stability\n",
    "        p = r + beta * p\n",
    "        rdotr = new_rdotr\n",
    "\n",
    "    return x\n",
    "\n",
    "def line_search(model: nn.Module,\n",
    "                compute_objective: Callable, # Function to compute surrogate loss\n",
    "                compute_constraint: Callable, # Function to compute KL divergence\n",
    "                current_params: torch.Tensor,\n",
    "                search_direction: torch.Tensor,\n",
    "                expected_improvement: float,\n",
    "                max_backtracks: int = 10,\n",
    "                accept_ratio: float = 0.1,\n",
    "                max_kl_constraint: float = 0.01) -> Tuple[bool, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Performs backtracking line search to find parameters that improve the objective\n",
    "    while satisfying the KL constraint.\n",
    "\n",
    "    Args:\n",
    "        model: The policy model.\n",
    "        compute_objective: Function that takes flat params and returns scalar objective value.\n",
    "        compute_constraint: Function that takes flat params and returns scalar constraint value (KL).\n",
    "        current_params: Flat parameters before the update.\n",
    "        search_direction: The proposed step direction (scaled).\n",
    "        expected_improvement: Expected improvement from the quadratic approximation.\n",
    "        max_backtracks: Maximum number of backtracking steps.\n",
    "        accept_ratio: Minimum ratio of actual_improvement / expected_improvement.\n",
    "        max_kl_constraint: The KL divergence limit.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (success, new_params). 'success' is True if a valid step is found.\n",
    "    \"\"\"\n",
    "    current_objective = compute_objective(current_params)\n",
    "\n",
    "    for step_frac in (0.5**np.arange(max_backtracks)):\n",
    "        new_params = current_params + step_frac * search_direction\n",
    "        set_flat_params_to(model, new_params) # Temporarily update model\n",
    "        \n",
    "        # Evaluate objective and constraint with new parameters\n",
    "        new_objective = compute_objective(new_params) # Needs scalar loss\n",
    "        kl_divergence = compute_constraint(new_params) # Needs scalar KL\n",
    "        \n",
    "        actual_improvement = new_objective - current_objective\n",
    "        expected_step_improvement = expected_improvement * step_frac\n",
    "        ratio = actual_improvement / (expected_step_improvement + 1e-8) # Add epsilon\n",
    "\n",
    "        # Check if improvement is sufficient and KL constraint is met\n",
    "        if ratio > accept_ratio and actual_improvement > 0 and kl_divergence <= max_kl_constraint:\n",
    "            # print(f\"Line search success: step_frac={step_frac:.4f}, KL={kl_divergence:.4f}, Impr={actual_improvement:.4f}\")\n",
    "            return True, new_params # Keep the temporary update\n",
    "\n",
    "    # If no step is found, revert to original parameters\n",
    "    set_flat_params_to(model, current_params)\n",
    "    # print(\"Line search failed.\")\n",
    "    return False, current_params\n",
    "\n",
    "def trpo_step(model: Policy, # Use the specific Policy type hint if available\n",
    "              value_net: Value, # Often needed for advantage calculation (outside this func)\n",
    "              states: torch.Tensor,\n",
    "              actions: torch.Tensor,\n",
    "              advantages: torch.Tensor,\n",
    "              old_log_prob: torch.Tensor, # Log probs from the policy *before* the update\n",
    "              max_kl: float = 1e-2,\n",
    "              damping: float = 1e-2):\n",
    "    \"\"\"\n",
    "    Performs a single TRPO policy update step.\n",
    "\n",
    "    Args:\n",
    "        model: The policy network to be updated.\n",
    "        value_net: The value network (potentially needed for helpers, though not directly used here).\n",
    "        states: Batch of states.\n",
    "        actions: Batch of actions corresponding to states.\n",
    "        advantages: Batch of advantages corresponding to state-action pairs.\n",
    "        old_log_prob: Log probabilities of the actions under the policy *before* this update.\n",
    "        max_kl: Maximum KL divergence constraint.\n",
    "        damping: Damping factor for the Fisher-vector product calculation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (loss_gradient, kl_divergence_before_update) - for logging/debugging.\n",
    "               The primary result is the update to the 'model' parameters.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are tensors\n",
    "    states = torch.as_tensor(states, dtype=torch.float32)\n",
    "    actions = torch.as_tensor(actions, dtype=torch.float32)\n",
    "    advantages = torch.as_tensor(advantages, dtype=torch.float32)\n",
    "    old_log_prob = torch.as_tensor(old_log_prob, dtype=torch.float32)\n",
    "\n",
    "    # --- 1. Calculate Surrogate Loss and Gradient ---\n",
    "    # Define functions needed for line search *before* calculating the initial gradient\n",
    "    # These functions take flat parameters as input\n",
    "    def compute_surrogate_loss(flat_params: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Computes the scalar surrogate loss for given parameters.\"\"\"\n",
    "        # Temporarily set model parameters\n",
    "        original_params = get_flat_params_from(model)\n",
    "        set_flat_params_to(model, flat_params)\n",
    "\n",
    "        # Calculate loss (ensure no gradient tracking needed here for line search eval)\n",
    "        with torch.no_grad():\n",
    "            log_prob = model.get_log_probability_density(states, actions)\n",
    "            # Importance sampling ratio: pi_new(a|s) / pi_old(a|s)\n",
    "            ratio = torch.exp(log_prob - old_log_prob)\n",
    "            surr_loss = - (advantages.squeeze() * ratio).mean() # Negative for minimization\n",
    "\n",
    "        # Restore original parameters\n",
    "        set_flat_params_to(model, original_params)\n",
    "        return surr_loss\n",
    "    \n",
    "    def compute_kl_divergence(flat_params: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Computes the scalar KL divergence for given parameters.\"\"\"\n",
    "        # Temporarily set model parameters\n",
    "        original_params = get_flat_params_from(model)\n",
    "        set_flat_params_to(model, flat_params)\n",
    "\n",
    "        # Calculate KL (ensure no gradient tracking needed here for line search eval)\n",
    "        # Assumes model.get_KL_divergence computes KL(old || new) using old_log_prob\n",
    "        # If get_KL_divergence needs the 'old' policy explicitly, adjust accordingly.\n",
    "        with torch.no_grad():\n",
    "            # Recompute current log_prob with new params\n",
    "            current_log_prob = model.get_log_probability_density(states, actions)\n",
    "            # KL divergence D_KL(pi_old || pi_new) = E_{a~pi_old} [log pi_old(a|s) - log pi_new(a|s)]\n",
    "            kl = (old_log_prob - current_log_prob).mean()\n",
    "\n",
    "        # Restore original parameters\n",
    "        set_flat_params_to(model, original_params)\n",
    "        return kl\n",
    "    \n",
    "    # Calculate the initial loss and gradient using current model parameters\n",
    "    log_prob = model.get_log_probability_density(states, actions)\n",
    "    ratio = torch.exp(log_prob - old_log_prob)\n",
    "    action_loss = - (advantages.squeeze() * ratio) # Surrogate objective term per sample\n",
    "    loss = action_loss.mean() # Average surrogate loss\n",
    "\n",
    "    # Calculate the gradient of the loss w.r.t. current model parameters\n",
    "    model.zero_grad() # Zero gradients before backward pass\n",
    "    loss.backward() # Compute gradients\n",
    "    loss_grad_list = [param.grad.clone() for param in model.parameters() if param.grad is not None]\n",
    "    loss_grad = torch.cat([grad.view(-1) for grad in loss_grad_list]).detach() # Flatten and detach\n",
    "\n",
    "    # --- 2. Fisher-Vector Product Calculation ---\n",
    "    # (Defined as an inner function for cleaner scope, using variables from trpo_step)\n",
    "    \n",
    "    def fisher_vector_product(v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the Fisher-vector product (FVP) F*v or (H + damping*I)*v.\n",
    "\n",
    "        Uses the Hessian of the KL divergence as an approximation for the Fisher\n",
    "        Information Matrix (FIM). The computation H*v is done efficiently via\n",
    "        two backward passes without explicitly constructing H.\n",
    "\n",
    "        Args:\n",
    "            v: The vector to multiply with the FIM (or damped Hessian). Should be\n",
    "                a flattened tensor with the same number of elements as model parameters.\n",
    "\n",
    "        Returns:\n",
    "            The Fisher-vector product (H + damping*I)*v, detached from the graph.\n",
    "        \"\"\"\n",
    "        model.zero_grad() # Ensure gradients are zeroed before computation\n",
    "\n",
    "        # 1. Calculate KL divergence KL(old || current)\n",
    "        # Ensure get_KL_divergence calculates the mean KL: E[log_old - log_new]\n",
    "        # Recompute current log prob *with gradients enabled* for FVP\n",
    "        current_log_prob_fvp = model.get_log_probability_density(states, actions)\n",
    "        kl = (old_log_prob - current_log_prob_fvp).mean() # KL(pi_old || pi_new)\n",
    "\n",
    "        # 2. Calculate first gradient: grad_kl = d(kl_div) / d(theta)\n",
    "        # retain_graph=True and create_graph=True is essential for the Hessian-vector product calculation\n",
    "        # They tell PyTorch to keep the computational graph that produced these gradients (g) intact, \n",
    "        # because we need to differentiate through this gradient calculation in a later step.\n",
    "        grads = torch.autograd.grad(kl, model.parameters(), retain_graph=True, create_graph=True)\n",
    "        flat_grad_kl = torch.cat([grad.view(-1) for grad in grads])\n",
    "\n",
    "        # Ensure v is on the correct device and dtype, and detached\n",
    "        v_device = flat_grad_kl.device\n",
    "        v_dtype = flat_grad_kl.dtype\n",
    "        v = v.to(device=v_device, dtype=v_dtype).detach()\n",
    "\n",
    "        # 3. Calculate dot product: gv = sum(grad_kl * v)\n",
    "        grad_kl_dot_v = torch.dot(flat_grad_kl, v)\n",
    "\n",
    "        # 4. Calculate second gradient (Hessian-vector product):\n",
    "        # Hv = d(grad_kl_dot_v) / d(theta) = d(dot(g, v)) / d(theta) = d(dot(d(kl)/d(theta), v)) / d(theta)\n",
    "        # H is the Hessian of the KL divergence (H = d^2(kl) / d(theta)^2)\n",
    "        # No need for create_graph=True or retain_graph=True here usually\n",
    "        hessian_vector_prod_tuple = torch.autograd.grad(grad_kl_dot_v, model.parameters())\n",
    "        # Flatten the result H*v\n",
    "        flat_hessian_vector_prod = torch.cat([\n",
    "            grad.contiguous().view(-1) for grad in hessian_vector_prod_tuple\n",
    "        ])\n",
    "\n",
    "        # 5. Apply damping and detach: FVP = H*v + damping * v\n",
    "        # Damping helps stabilize optimization algorithms like Natural Gradient Descent or TRPO \n",
    "        # that use this calculation, preventing issues if H is ill-conditioned\n",
    "        fvp = flat_hessian_vector_prod.detach() + damping * v\n",
    "        # v is already detached from the earlier step\n",
    "\n",
    "        return fvp\n",
    "    \n",
    "    # --- 3. Conjugate Gradient ---\n",
    "    # Solve F * step_dir = -loss_grad (approximately F^{-1} * -loss_grad)\n",
    "    step_dir = conjugate_gradient(fisher_vector_product, -loss_grad, 10)\n",
    "    \n",
    "    # --- 4. Calculate Proposed Step Size ---\n",
    "    # Calculate s^T * F * s (where s = step_dir)\n",
    "    # This is needed to scale the step to meet the KL constraint\n",
    "    fvp_step_dir = fisher_vector_product(step_dir)\n",
    "    shs = 0.5 * torch.dot(step_dir, fvp_step_dir) # 0.5 * s^T * F * s\n",
    "    # Add epsilon to prevent division by zero or instability near zero\n",
    "    lagrange_multiplier = torch.sqrt(shs / (max_kl + 1e-8))\n",
    "    fullstep = step_dir / (lagrange_multiplier + 1e-8) # Scaled step: s / lm\n",
    "\n",
    "    # Calculate expected improvement: g^T * s (where g = -loss_grad, s = fullstep)\n",
    "    # Note: Original `loss_grad` is d(loss)/d(theta), so we use -loss_grad for improvement direction\n",
    "    neggdotstepdir = torch.dot(-loss_grad, step_dir)\n",
    "    expected_improvement = torch.dot(-loss_grad, fullstep) # Should be positive if step_dir is descent direction\n",
    "\n",
    "    # --- 5. Line Search ---\n",
    "    prev_params = get_flat_params_from(model)\n",
    "    initial_kl = compute_kl_divergence(prev_params) # For logging\n",
    "\n",
    "    success, new_params = line_search(\n",
    "        model,\n",
    "        compute_surrogate_loss, # Pass the function that computes scalar loss\n",
    "        compute_kl_divergence,  # Pass the function that computes scalar KL\n",
    "        prev_params,\n",
    "        fullstep,\n",
    "        expected_improvement,\n",
    "        max_kl_constraint=max_kl\n",
    "    )\n",
    "\n",
    "    # --- 6. Update Policy ---\n",
    "    set_flat_params_to(model, new_params) # Update model with parameters found by line search\n",
    "\n",
    "    # Return initial gradient and KL for logging purposes\n",
    "    return loss_grad, initial_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7851c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8c026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/69/6ry3j72176v59y5slnc712gw0000gn/T/ipykernel_46675/246926693.py:256: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  \"rewards\": torch.tensor(batch_rewards, dtype=torch.float32).unsqueeze(1), # Add dim for [N, 1]\n",
      "/var/folders/69/6ry3j72176v59y5slnc712gw0000gn/T/ipykernel_46675/246926693.py:103: UserWarning: Using a target size (torch.Size([4000, 1, 1])) that is different to the input size (torch.Size([1, 4000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = nn.functional.mse_loss(values_pred, returns) # Use torch MSE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tSteps Collected: 4000\tLast Ep Reward: -1211.06\tAvg Batch Ep Reward: -1324.67\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 64\n",
    "gamma = 0.99 # Discount factor\n",
    "tau = 0.95 # GAE lambda parameter\n",
    "l2_reg = 1e-3 # L2 regularization strength for value net\n",
    "max_kl: float = 0.01 # Max KL constraint for TRPO\n",
    "damping: float = 0.1 # Damping for FVP in TRPO\n",
    "policy_net = Policy(num_inputs, hidden_dim, num_actions)\n",
    "value_net = Value(num_inputs, hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "def update_policy(batch: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Updates the policy and value networks using GAE and TRPO.\n",
    "\n",
    "    Args:\n",
    "        batch: A dictionary containing 'states', 'actions', 'rewards', 'mask'.\n",
    "               Assumes data corresponds to a single trajectory or episode.\n",
    "    \"\"\"\n",
    "    # --- 1. Data Preparation ---\n",
    "    # Consider device placement (.to(device)) if using GPU\n",
    "    # Using torch.as_tensor is generally safer than torch.FloatTensor\n",
    "    # Avoid squeeze(0) unless batch structure guarantees dim 0 is size 1.\n",
    "    # Assume batch contains data for N steps: [N, state_dim], [N, action_dim], etc.\n",
    "    try:\n",
    "        states = torch.as_tensor(batch[\"states\"], dtype=torch.float32)\n",
    "        actions = torch.as_tensor(batch[\"actions\"], dtype=torch.float32)\n",
    "        rewards = torch.as_tensor(batch[\"rewards\"], dtype=torch.float32)\n",
    "        # Ensure masks are treated as floats for multiplication\n",
    "        masks = torch.as_tensor(batch[\"mask\"], dtype=torch.float32)\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Batch dictionary missing key: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch tensors: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Validate shapes - assuming [N, dim] format after potential loading squeeze\n",
    "    if states.dim() == 1: states = states.unsqueeze(0) # Handle single step case\n",
    "    if actions.dim() == 1: actions = actions.unsqueeze(0)\n",
    "    if rewards.dim() == 1: rewards = rewards.unsqueeze(0)\n",
    "    if masks.dim() == 1: masks = masks.unsqueeze(0)\n",
    "\n",
    "    # Ensure rewards and masks have a trailing dimension for broadcasting if needed\n",
    "    if rewards.dim() == 1: rewards = rewards.unsqueeze(-1) # Shape [N, 1]\n",
    "    if masks.dim() == 1: masks = masks.unsqueeze(-1)     # Shape [N, 1]\n",
    "    if actions.dim() == 1: actions = actions.unsqueeze(-1) # Shape [N, 1] if action_dim is 1\n",
    "\n",
    "    \n",
    "    # --- 2. Value Function Estimation ---\n",
    "    with torch.no_grad(): # No gradients needed for calculating targets\n",
    "        # Typo corrected: squeeeze -> squeeze\n",
    "        # Use squeeze(-1) if value_net outputs [N, 1], or just ensure output is [N]\n",
    "        values = value_net(states).squeeze(0) # Assuming output [N, 1] -> [N]\n",
    "\n",
    "\n",
    "    # --- 3. GAE and Returns Calculation ---\n",
    "    num_steps = rewards.size(0)\n",
    "    returns = torch.zeros_like(rewards)     # Use zeros_like for correct shape/device/dtype\n",
    "    deltas = torch.zeros_like(rewards)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "\n",
    "    prev_return = 0.0\n",
    "    prev_value = 0.0\n",
    "    prev_advantage = 0.0\n",
    "    for i in reversed(range(num_steps)):\n",
    "    # Ensure rewards[i], masks[i], values[i] are scalars or broadcastable\n",
    "        # Using .item() might be safer if shapes are guaranteed [1], but indexing should work for [N]\n",
    "        current_reward = rewards[i]\n",
    "        current_mask = masks[i]\n",
    "        current_value = values[i] # From value_net(states) calculated earlier\n",
    "\n",
    "        # Calculate return G(t) = r_t + gamma * G(t+1) * mask\n",
    "        returns[i] = current_reward + gamma * prev_return * current_mask\n",
    "\n",
    "        # Calculate TD error (delta) = r_t + gamma * V(s_{t+1}) * mask - V(s_t)\n",
    "        # Note: prev_value holds V(s_{t+1}) from the previous iteration\n",
    "        deltas[i] = current_reward + gamma * prev_value * current_mask - current_value\n",
    "\n",
    "        # Calculate GAE advantage A(t) = delta_t + gamma * tau * A(t+1) * mask\n",
    "        advantages[i] = deltas[i] + gamma * tau * prev_advantage * current_mask\n",
    "\n",
    "        # Update values for the next iteration (t-1)\n",
    "        # Use detach() instead of .data if accessing tensors that might have history\n",
    "        prev_return = returns[i].item() # Use .item() for scalar python number\n",
    "        prev_value = current_value.item() # V(s_t) becomes V(s_{t+1}) for next step\n",
    "        prev_advantage = advantages[i].item()\n",
    "\n",
    "    # --- 4. Value Function Update (using SciPy L-BFGS) ---\n",
    "    # Define the loss+gradient function required by fmin_l_bfgs_b\n",
    "    def get_value_loss_and_grad(flat_params_numpy):\n",
    "        # Convert numpy array back to tensor\n",
    "        flat_params = torch.tensor(flat_params_numpy, dtype=torch.float32) # Match model dtype\n",
    "        set_flat_params_to(value_net, flat_params)\n",
    "\n",
    "        # Zero gradients using standard method\n",
    "        value_net.zero_grad()\n",
    "\n",
    "        # Forward pass - ensure states is correctly shaped\n",
    "        values_pred = value_net(states) # Output shape likely [N, 1]\n",
    "\n",
    "        # Calculate MSE loss - ensure targets (returns) have compatible shape [N, 1]\n",
    "        value_loss = nn.functional.mse_loss(values_pred, returns) # Use torch MSE\n",
    "\n",
    "        # Add L2 regularization\n",
    "        l2_penalty = 0.0\n",
    "        for param in value_net.parameters():\n",
    "            l2_penalty += param.pow(2).sum()\n",
    "        value_loss += l2_penalty * l2_reg * 0.5 # Common to scale L2 by 0.5\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        value_loss.backward()\n",
    "\n",
    "        # Get flat gradient and convert back to numpy double\n",
    "        flat_grad = get_flat_grad_from(value_net)\n",
    "        # Return loss and gradient as numpy doubles\n",
    "        return value_loss.item(), flat_grad.cpu().numpy().astype(np.float64)\n",
    "\n",
    "    # Get initial parameters as numpy double\n",
    "    initial_params_numpy = get_flat_params_from(value_net).cpu().numpy().astype(np.float64)\n",
    "\n",
    "\n",
    "    # Optimize using L-BFGS-B\n",
    "    try:\n",
    "        optimal_params_numpy, _, _ = scipy.optimize.fmin_l_bfgs_b(\n",
    "            get_value_loss_and_grad, initial_params_numpy, maxiter=25 # Adjust maxiter as needed\n",
    "        )\n",
    "        # Update the value network with the optimized parameters\n",
    "        set_flat_params_to(value_net, torch.tensor(optimal_params_numpy, dtype=torch.float32))\n",
    "    except Exception as e:\n",
    "        print(f\"L-BFGS optimization failed: {e}\")\n",
    "        # Decide how to handle failure (e.g., skip update, use old params)\n",
    "\n",
    "\n",
    "    # --- 5. Advantage Normalization ---\n",
    "    # Ensure advantages tensor has requires_grad=False before in-place ops if needed elsewhere\n",
    "    advantages = advantages.detach() # Make sure it's detached before normalization\n",
    "    adv_mean = advantages.mean()\n",
    "    adv_std = advantages.std() + 1e-8 # Add epsilon for numerical stability\n",
    "    normalized_advantages = (advantages - adv_mean) / adv_std\n",
    "\n",
    "    # --- 6. Prepare for TRPO Step ---\n",
    "    # Calculate log probabilities using the policy *before* the update\n",
    "    with torch.no_grad(): # Ensure no gradients are computed here\n",
    "        action_means, action_log_stds, _ = policy_net(states)\n",
    "        # Use the assumed log density function or the model's method\n",
    "        # Ensure actions tensor has the correct shape expected by the function\n",
    "        old_log_prob = normal_log_density(actions, action_means, action_log_stds)\n",
    "        # No need for .data.clone(), detach() is sufficient if needed,\n",
    "        # but with torch.no_grad(), it's already detached.\n",
    "\n",
    "    \n",
    "    # --- 7. Perform TRPO Step ---\n",
    "    # Ensure the trpo_step function signature matches the arguments provided\n",
    "    # Pass necessary arguments like max_kl and damping\n",
    "    loss_grad, kl_divergence = trpo_step(\n",
    "        model=policy_net,\n",
    "        value_net=value_net, # Pass if needed by helpers within trpo_step\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        advantages=normalized_advantages, # Use normalized advantages\n",
    "        old_log_prob=old_log_prob,\n",
    "        max_kl=max_kl,     # Pass the constraint value\n",
    "        damping=damping    # Pass the damping value\n",
    "    )\n",
    "\n",
    "    \n",
    "n_episodes = 1000 # Example total episodes\n",
    "batch_size = 4000  # Target number of steps per policy update batch\n",
    "max_episode_steps = 1000 # Max steps per episode\n",
    "log_interval = 100 # How often to print logs\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "total_steps_processed = 0 # Keep track of total steps across all updates\n",
    "\n",
    "rewards = []\n",
    "\n",
    "for i_episode in range(n_episodes):\n",
    "    # Data storage for the current batch (will collect multiple episodes)\n",
    "    batch_states = []\n",
    "    batch_actions = []\n",
    "    batch_rewards = []\n",
    "    batch_masks = [] # Represents (1 - done)\n",
    "\n",
    "    steps_in_batch = 0\n",
    "    episodes_in_batch = 0\n",
    "    total_reward_in_batch = 0.0\n",
    "\n",
    "    # Collect experience until batch_size is reached\n",
    "    while steps_in_batch < batch_size:\n",
    "        state = env.reset()\n",
    "        # Ensure state is in the format expected by policy_net (e.g., numpy array)\n",
    "        # If policy_net expects a tensor, convert here:\n",
    "        # state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "\n",
    "        # Temporary storage for the current episode's trajectory\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        episode_masks = []\n",
    "\n",
    "        for t in range(max_episode_steps):\n",
    "            # 1. Get Action\n",
    "            # Ensure state format matches policy_net.get_action input requirement\n",
    "            # Assuming get_action returns a tensor\n",
    "            state_tensor = torch.from_numpy(state.reshape(-1)).float().unsqueeze(0)\n",
    "            action_tensor = policy_net.get_action(state_tensor)\n",
    "            # Convert action to numpy for the environment step if needed\n",
    "            action_numpy = action_tensor.detach().cpu().numpy() # Adjust based on env requirements\n",
    "\n",
    "            # 2. Step Environment\n",
    "            # Ensure env.step returns consistent types (usually numpy for state/reward)\n",
    "            next_state, reward, done, _ = env.step(action_numpy)\n",
    "\n",
    "            # 3. Store Transition Data (using consistent types, e.g., numpy for states)\n",
    "            episode_states.append(state.reshape(-1)) # Store original state (numpy)\n",
    "            episode_actions.append(action_tensor) # Store action tensor\n",
    "            episode_rewards.append(reward) # Store reward (float/numpy)\n",
    "            episode_masks.append(1.0 - float(done)) # Store mask (float)\n",
    "\n",
    "            state = next_state # Update state for next iteration\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # End of episode: Append episode data to the main batch lists\n",
    "        batch_states.extend(episode_states)\n",
    "        batch_actions.extend(episode_actions)\n",
    "        batch_rewards.extend(episode_rewards)\n",
    "        batch_masks.extend(episode_masks)\n",
    "\n",
    "        # Update batch counters\n",
    "        steps_in_batch += episode_steps\n",
    "        total_reward_in_batch += episode_reward\n",
    "        episodes_in_batch += 1\n",
    "\n",
    "        # Store the reward of the *last completed* episode for logging\n",
    "        last_episode_reward = episode_reward\n",
    "\n",
    "    # --- Batch Finalization and Policy Update ---\n",
    "    # Calculate average reward per episode in this batch\n",
    "    avg_reward_per_episode = total_reward_in_batch / episodes_in_batch if episodes_in_batch > 0 else 0.0\n",
    "    total_steps_processed += steps_in_batch\n",
    "\n",
    "    # Prepare batch dictionary for update_policy\n",
    "    # Convert lists of data points into single tensors\n",
    "    # Ensure correct dtypes and device placement (.to(device)) if using GPU\n",
    "    update_batch = {\n",
    "        \"states\": torch.tensor(np.asarray(batch_states), dtype=torch.float32),\n",
    "        \"actions\": torch.stack(batch_actions), # Stack list of action tensors\n",
    "        \"rewards\": torch.tensor(batch_rewards, dtype=torch.float32).unsqueeze(1), # Add dim for [N, 1]\n",
    "        \"mask\": torch.tensor(batch_masks, dtype=torch.float32).unsqueeze(1)      # Add dim for [N, 1]\n",
    "        # Note: \"next_states\" is often not needed directly by GAE/TRPO update,\n",
    "        # but if it were, you'd collect and tensorize it similarly.\n",
    "    }\n",
    "\n",
    "    # Call the policy update function\n",
    "    update_policy(update_batch) # Pass the correctly formatted batch\n",
    "    rewards.append(avg_reward_per_episode[0])\n",
    "    # --- Logging ---\n",
    "    if i_episode % log_interval == 0:\n",
    "        print(f'Episode {i_episode}\\tSteps Collected: {steps_in_batch}\\t'\n",
    "              f'Last Ep Reward: {last_episode_reward[0]:.2f}\\t'\n",
    "              f'Avg Batch Ep Reward: {avg_reward_per_episode[0]:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
