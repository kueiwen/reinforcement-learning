{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b57ed8",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning Concept**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822e086",
   "metadata": {},
   "source": [
    " Reinforcement learning (RL) is one of category of machine learning (ML). However, RL Not like other ML methods, RL does not need labels or rules, the agent of RL interact with the environment and get reward from the environment, which is the target to optimize. \n",
    "\n",
    "<img src=\"img/reinforcement_learning.png\" width=\"600\">\n",
    "\n",
    "1. environment tells current state\n",
    "\n",
    "2. agent makes an action based on current state\n",
    "\n",
    "3. enviroment return the reward and next state according to the action\n",
    "\n",
    "We can see that here are 3 infromation delivered between agent and environment: state, action and reward. With respect to the action $(a)$, current state $(S_t)$ will switch to next state $(S_{t+1})$. If the environment is fully observable, which means that all $P(S_{t+1}|S_t=s,A=a)$ are known, we said that the state fulfill Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95807a55",
   "metadata": {},
   "source": [
    "## **Markov Decision Process**\n",
    "\n",
    "Markov decision process (MDP) is an imprtant concept regarding reinforcement learning (RL), almost all RL probelms can be modeled as MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a95e34",
   "metadata": {},
   "source": [
    "### **1. Markov Property**\n",
    "\n",
    "Markov property illstrate distribution of future state, the future is only depends on current state. which means that current state can represent all past.\n",
    "\n",
    "$P(S_{t+1}|S_1,...S_t)=P(S_{t+1}|S_t)$\n",
    "\n",
    "Here is an example of state transition,\n",
    "\n",
    "\n",
    "<img src=\"img/markov_process.png\" width=\"600\">\n",
    "\n",
    "By Markov process, if current state $S_t$ is Standing, than   \n",
    "$P(S_{t+1}=\\text{Sitting}|S_{t}=\\text{Standing})=0.3$  \n",
    "$P(S_{t+1}=\\text{Hands Raised}|S_{t}=\\text{Standing})=0.1$  \n",
    "$P(S_{t+1}=\\text{Foot Forward}|S_{t}=\\text{Standing})=0.5$  \n",
    "$P(S_{t+1}=\\text{Shut Down}|S_{t}=\\text{Standing})=0.1$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e32c47",
   "metadata": {},
   "source": [
    "### **2. Markov Reward Process**\n",
    "\n",
    "If we add **reward** to the state transtion, here comes a Markov reward process.  \n",
    "Markov reward process is a tuple $<S, P, R, \\gamma>$\n",
    "\n",
    "$S$: finite set of states  \n",
    "$P$: state transition probability matrix $P_{ss^{\\prime}}=P(S_{t+1}=s^{\\prime}|S_t=s)$  \n",
    "$R$: reward function $R_s=E(R_{t+1}|S_t=s)$  \n",
    "$\\gamma$: discount factor, $\\gamma \\in [0,1]$\n",
    "\n",
    "Here is an example of Makov Reward Process based on previous Makov process example. \n",
    "\n",
    "<img src=\"img/markov_reward_process.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2554a",
   "metadata": {},
   "source": [
    "Similarly, if current state $S_t$ is Standing, than\n",
    "\n",
    "$R_{S_{t}=\\text{Standing}}=E(R_{t+1}|S_{t}=\\text{Standing})$  \n",
    "$=0.6\\times(-1)+0.1\\times0.1+0.5\\times(-1)+0.1\\times0$  \n",
    "$=-0.6+0.01=0.5=-0.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb877793",
   "metadata": {},
   "source": [
    "#### **Discount Factor**\n",
    "\n",
    "We usually prefer near reward due to uncertainty in the future, so the more future reward will get less value than it's actual reward.\n",
    "\n",
    "$G_t=R_{t+1}+\\gamma R_{t+2}+...=\\sum_{=0}^{\\infty}\\gamma^kR_{t+k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33873b5",
   "metadata": {},
   "source": [
    "#### **Bellman Equation for MRP**\n",
    "\n",
    "Bellman equation is to calculate the state value, which not only contain reward, but discounted value of current state.\n",
    "\n",
    "$v(s)=E(G_t|S_t=s)$\n",
    "    $=E(R_{t+1}+\\gamma R_{t+2}+...|S_t=s)$  \n",
    "    $=E(R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}...)|S_t=s)$  \n",
    "    $=E(R_{t+1}+\\gamma G_{t+1}|S_t=s)$  \n",
    "    $=E(R_{t+1}+\\gamma v{S_{t+1}}|S_t=s)$\n",
    "\n",
    "<img src=\"img/mrp_state_value.png\" width=\"400\">\n",
    "\n",
    "So if $\\gamma=1$, which mean there is no discount,  \n",
    "\n",
    "$v(\\text{Standing})=-1+R_{S_{t}}=-1+(-0.99)=-1.99$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94bb041",
   "metadata": {},
   "source": [
    "### **3. Markov Decision Process**\n",
    "\n",
    "If add **Action** into Markov Reward Process, then will become Markov Decision Process, which is a tuple $<S,A,P,R,\\gamma>$\n",
    "\n",
    "$S$: finite set of states  \n",
    "$A$: finite set of actions \n",
    "$P$: state transition probability matrix $P_{ss^{\\prime}}^a=P(S_{t+1}=s^{\\prime}|S_t=s, A_t=a)$  \n",
    "$R$: reward function $R_s^a=E(R_{t+1}|S_t=s,A_t=a)$  \n",
    "$\\gamma$: discount factor, $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6a2b7",
   "metadata": {},
   "source": [
    "#### **Policy**\n",
    "\n",
    "A policy is a distribution over actions with a given state.\n",
    "\n",
    "$\\pi(a|s)=P(A_t=a|S_t=s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef25769",
   "metadata": {},
   "source": [
    "#### **Bellman Equation for MDP**\n",
    "\n",
    "State value function: $v_{\\pi}(s)=E_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S+t=s]$\n",
    "\n",
    "Action value function: $q_{\\pi}(s,a)=E_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1},A_{t+1})|S+t=s,A_t=a]$\n",
    "\n",
    "<img src=\"img/mdp_state_value.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa318423",
   "metadata": {},
   "source": [
    "The relation between state and action function:  \n",
    "\n",
    "$v_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)q_{\\pi}(s,a)$\n",
    "\n",
    "$\\implies$ state value is the expected value over action value.\n",
    "\n",
    "$q_{\\pi}(s,a)=R_s^a+\\gamma\\sum_{s^{\\prime}\\in S}P_{ss^{\\prime}}^a v_{\\pi}(s^{\\prime})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95880e1c",
   "metadata": {},
   "source": [
    "## **Optimal Value Function**\n",
    "\n",
    "<img src=\"img/optimal_state_value.png\" width=\"800\">\n",
    "\n",
    "\n",
    "If we further expand to next step, we can get the optimal equaiton,\n",
    "\n",
    "$v_{*}(s)=\\max_{\\pi}v_{\\pi}(s)=\\max_{a}R_s^a+\\gamma\\sum_{s^{\\prime}\\in S}P_{ss^{\\prime}}^{a}v_{*}(s^{\\prime})$  \n",
    "$q_{*}(s,a)=\\max_{\\pi}q_{\\pi}(s,a)=R_s^a+\\gamma\\sum_{s^{\\prime}\\in S}P_{ss^{\\prime}}^{a}\\max_{a^{\\prime}}q_{*}(s^{\\prime},a^{\\prime})$  \n",
    "\n",
    "$\\pi_{*}(a|s)=\\begin{cases} 1\\quad \\text{if}\\ a=\\text{argmax}_{a\\in A}q_{*}(s,a) \\\\ 0\\quad \\text{otherwise}\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb206ef",
   "metadata": {},
   "source": [
    "Here are 2 method to find the optimal policy: **policy iteration** and **value iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35109a33",
   "metadata": {},
   "source": [
    "### **1. Policy iteration**\n",
    "\n",
    "Policy iteration is iteratively working on **policy evaluation** to update the value of each state and **policy improvement** by greedy method until optimal policy is achieved, in other word, there is no other policy is better than current policy. \n",
    "\n",
    "<img src=\"img/policy_iteration.png\" width=\"600\">\n",
    "\n",
    "The following equation is used to update the value of each state:\n",
    "\n",
    "$v_{k+1}=\\sum_{a\\in A}\\pi(a|s)(R_s^a+\\gamma\\sum_{ss^{\\prime}}^a v_k(s^{\\prime}))$\n",
    "\n",
    "$v_{k+1}=R^{\\pi}+\\gamma P^{\\pi}v_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6724a4",
   "metadata": {},
   "source": [
    "Following is the algorithm of policy iteration: \n",
    "\n",
    "***Policy Iteration***\n",
    "\n",
    "---\n",
    "**Input** MDP $M=<S,s_0,A,P_a(s^{\\prime},s),r(s,a,s^{\\prime}),\\gamma>$  \n",
    "**Output** $\\pi^{*}(s),\\quad \\forall s\\in S$\n",
    "\n",
    "**1. Initialization:** $V(s)\\in\\Bbb{R}$ and $\\pi(s)$ arbitrarily $\\forall s\\in S$\n",
    "\n",
    "**2. Policy Evaluation**  \n",
    "**repeat**    \n",
    "    $\\quad\\Delta\\leftarrow 0$      \n",
    "    $\\quad$**for each** $s\\in S$    \n",
    "        $\\quad\\quad V_{\\pi}^{\\prime}\\leftarrow \\sum_{s^{\\prime}\\in S}P_{a=\\pi(a|s)}(s^{\\prime},s)[r(s,\\pi(a|s),s^{\\prime})+\\gamma V_{\\pi}(s^{\\prime})]$       \n",
    "        $\\quad\\quad\\Delta\\leftarrow \\max(\\Delta,|V_{\\pi}^{\\prime}-V_{\\pi}|)$     \n",
    "    $\\quad$**end**   \n",
    "    $\\quad V_{\\pi}\\leftarrow V_{\\pi}^{\\prime}$    \n",
    "**until** $\\Delta\\leq\\epsilon$\n",
    "\n",
    "**3. Policy Improvement**    \n",
    "$stable\\leftarrow true$     \n",
    "**for each** $s\\in S$    \n",
    "    $\\quad\\pi^{\\prime}(s)\\leftarrow\\text{argmax}_a \\sum_{s^{\\prime\\in S}}\\sum_{s^{\\prime}\\in S}P_{a=\\pi(a|s)}(s^{\\prime},s)[r(s,\\pi(a|s),s^{\\prime})+\\gamma V_{\\pi}(s^{\\prime})]$     \n",
    "    $\\quad$**if** $\\pi(s)\\neq\\pi^{\\prime}(s)$      \n",
    "        $\\quad\\quad stable\\leftarrow false$     \n",
    "    $\\quad$**end**       \n",
    "**end**    \n",
    "**if** $stable=true$      \n",
    "    $\\quad$**return** $V$ and $\\pi$  \n",
    "**else**    \n",
    "    $\\quad$ repeat step 2     \n",
    "**end**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "The $\\epsilon$ here is pre-defined the value difference to previous iteration, when the differnce is small enough, then can go to policy improvement step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d99db",
   "metadata": {},
   "source": [
    "For example, in this 4X4 grid, we need to go to the end point with less steps, the upper left corner case and the lower right corner case are the end points. \n",
    "\n",
    "<img src=\"img/policy_iteration_example.png\" width=\"400\">  \n",
    "\n",
    "The reward of each step is $-1$, and there are 4 actions can be chose: up, down, right and left. Initilly the pobability of each action are all the same to be $0.25$.  \n",
    "\n",
    "<img src=\"img/policy_iteration_example_solution.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f79a1",
   "metadata": {},
   "source": [
    "$k$ is the number of iteration, so when $k=0$ at initial place, all states are with $0$ value.\n",
    "\n",
    "When $k=1$, we take an action at each state, no matter at which case will get $-1$ reward,  \n",
    "* For the 1st case, the current reward is $-1$, and the expected future value is $0\\times0.25+0\\times0.25+0\\times0.25+0\\times0.25=0$, so the total value is $-1$.\n",
    "\n",
    "When $k=2$, we need to consider the reward get from last iteration and this iteration,   \n",
    "* For the 1st case, the current reward is $-1$, and the expected future value is $(-1)\\times0.25+(-1)\\times0.25+(-1)\\times0.25+0\\times0.25=-0.75$, so the total value is $-1+(-0.75)=-1.75$ (because there is no upper case, so calculate by staying)\n",
    "* For the 5th case, the current reward is $-1$, and the expected future value is $(-1)\\times0.25+(-1)\\times0.25+(-1)\\times0.25+(-1)\\times0.25=-1$, so the total value is $-1+(-1)=-2$\n",
    "\n",
    "When $k=3$, use the same logic as $k=2$,  \n",
    "* For the 1st case, the current reward is $-1$, and the expected future value is $(-1.75)\\times0.25+(-2)\\times0.25+(-2)\\times0.25+0\\times0.25=-1.4375$, so the total value is $-1+(-1.4375)=-2.4375$ (because there is no upper case, so calculate by staying)\n",
    "* For the 5th case, the current reward is $-1$, and the expected future value is $(-1.75)\\times0.25+(-2)\\times0.25+(-2)\\times0.25+(-1.75)\\times0.25=-1.875$, so the total value is $-1+(-1.875)=-2.875$\n",
    "\n",
    "\n",
    "When $k>3$, can see that even state values are different, but the policy is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41931f52",
   "metadata": {},
   "source": [
    "To implement policy interation in code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ef42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation\n",
      "Policy Improvement\n",
      "Policy Evaluation\n",
      "Policy Improvement\n",
      "Final value function:\n",
      "[  0.          -8.92364362 -12.47781364 -13.5822362   -8.92364362\n",
      " -11.37339108 -12.52806075 -12.47781364 -12.47781364 -12.52806075\n",
      " -11.37339108  -8.92364362 -13.5822362  -12.47781364  -8.92364362\n",
      "   0.        ]\n",
      "Optimal policy:\n",
      "State 1: ['left']\n",
      "State 2: ['left']\n",
      "State 3: ['down', 'left']\n",
      "State 4: ['up']\n",
      "State 5: ['up', 'left']\n",
      "State 6: ['down', 'left']\n",
      "State 7: ['down']\n",
      "State 8: ['up']\n",
      "State 9: ['up', 'right']\n",
      "State 10: ['down', 'right']\n",
      "State 11: ['down']\n",
      "State 12: ['up', 'right']\n",
      "State 13: ['right']\n",
      "State 14: ['right']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Step 1: Initialization\n",
    "def initalize():\n",
    "    #pi[state] = [up, down, left, right, end]\n",
    "    pi = dict()\n",
    "    pi[0] = [0, 0, 0, 0]\n",
    "    for i in range(1,15):\n",
    "        pi[i] = [0.25, 0.25, 0.25, 0.25]\n",
    "    pi[15] = [0, 0, 0, 0]\n",
    "\n",
    "    v = np.zeros(16)\n",
    "\n",
    "    q = np.zeros((16, 4))\n",
    "    return pi, v, q\n",
    "\n",
    "\n",
    "def transition(state, action):\n",
    "    if state in [0, 15]:\n",
    "        return state\n",
    "    if action == 0: # up\n",
    "        if state in [1, 2, 3]:\n",
    "            return state\n",
    "        else:\n",
    "            return state - 4\n",
    "    if action == 1: # down\n",
    "        if state in [12, 13, 14]:\n",
    "            return state\n",
    "        else:\n",
    "            return state + 4\n",
    "    if action == 2: # left\n",
    "        if state in [4, 8, 12]:\n",
    "            return state\n",
    "        else:\n",
    "            return state - 1\n",
    "    if action == 3: # right\n",
    "        if state in [3, 7, 11]:\n",
    "            return state\n",
    "        else:\n",
    "            return state + 1\n",
    "        \n",
    "\n",
    "# Step 2: Policy Evaluation\n",
    "def policy_evaluation():\n",
    "    global pi, v, q\n",
    "    print(\"Policy Evaluation\")\n",
    "    epsilon = 0.5\n",
    "    while True:\n",
    "        v_prime = np.zeros(16)\n",
    "        delta = 0\n",
    "        for k in range(1,15):\n",
    "            for j in range(4):\n",
    "                next_state = transition(k, j)\n",
    "                v_prime[k] += 0.25 * (-1 + v[next_state])\n",
    "                q[k][j] = 0.25 * (-1 + v[next_state])\n",
    "        delta = max(delta, np.max(abs(v_prime - v)))\n",
    "        v = v_prime\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "\n",
    "#Step 3: Policy Improvement\n",
    "def policy_improvement():\n",
    "    global pi, q\n",
    "    print(\"Policy Improvement\")\n",
    "    stable = True\n",
    "    for k in range(1,15):\n",
    "        argmax = np.where(q[k] == np.max(q[k]))[0]\n",
    "        p = np.zeros(4)\n",
    "        p[argmax] = 1/len(argmax)\n",
    "        for j in range(4):\n",
    "            if pi[k][j] != p[j]:\n",
    "                stable = False\n",
    "        pi[k] = p\n",
    "    return stable\n",
    "\n",
    "\n",
    "def policy_iteration():\n",
    "    while True:\n",
    "        policy_evaluation()\n",
    "        stable = policy_improvement()\n",
    "        if stable:\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def mapping_policy(pi):\n",
    "    policy = dict()\n",
    "    for k in range(1, 15):\n",
    "        idx = np.where(pi[k] == np.max(pi[k]))[0]\n",
    "        policy[k] = []\n",
    "        if 0 in idx:\n",
    "            policy[k].append('up')\n",
    "        if 1 in idx:\n",
    "            policy[k].append('down')\n",
    "        if 2 in idx:\n",
    "            policy[k].append('left')\n",
    "        if 3 in idx:\n",
    "            policy[k].append('right')\n",
    "    return policy\n",
    "pi, v, q = initalize()\n",
    "policy_iteration()\n",
    "policy = mapping_policy(pi)\n",
    "print(\"Final value function:\")\n",
    "print(v)\n",
    "print(\"Optimal policy:\")\n",
    "for k in range(1, 15):\n",
    "    print(f\"State {k}: {policy[k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06d8d8",
   "metadata": {},
   "source": [
    "### **2. Value iteration**\n",
    "\n",
    "Value iteration is a dymanic programming method for finding he optimal value $V^{*}$ by solving Bellman equation iteratively.\n",
    "\n",
    "Following is the algorithm of value iteration: to repeatly calculate $V$ using he BEllman function until we converge to the solution or the limited iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8df36d",
   "metadata": {},
   "source": [
    "***State Value Equation***\n",
    "\n",
    "---\n",
    "**Input** MDP $M=<S,s_0,A,P_a(s^{\\prime},s),r(s,a,s^{\\prime}),\\gamma>$  \n",
    "**Output** $\\pi^{*}(s),\\quad \\forall s\\in S$\n",
    "\n",
    "Set $V$ to arbitrary value function; e.g., $V(s)=0$ for all $s$\n",
    "\n",
    "**repeat**  \n",
    "    $\\quad\\Delta \\leftarrow 0$  \n",
    "    $\\quad$**for each** $s\\in S$  \n",
    "        $\\quad\\quad V^{\\prime}(s)\\leftarrow \\max_{a\\in A(s)}\\sum_{s^{\\prime}\\in S}P_a(s^{\\prime}|s)[r(s,a,s^{\\prime})+\\gamma V(s^{\\prime})]$  $\\leftarrow$ Bellman equation  \n",
    "        $\\quad\\quad\\Delta \\leftarrow \\max(\\Delta,|V^{\\prime}(s)-V(s)|)$   \n",
    "    $\\quad$**end**   \n",
    "    $\\quad V\\leftarrow V^{\\prime}$  \n",
    "**until** $\\Delta \\leq \\epsilon$\n",
    "\n",
    "**for each** $s\\in S$  \n",
    "    $\\quad\\pi^{*}(s)\\leftarrow\\text{argmax}_a \\sum_{s^{\\prime}\\in S}P_a(s^{\\prime}|s)[r(s,a,s^{\\prime})+\\gamma V(s^{\\prime})]$    \n",
    "**end**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638ff5d",
   "metadata": {},
   "source": [
    "***Action Value Equation***\n",
    "\n",
    "---\n",
    "**Input** MDP $M=<S,s_0,A,P_a(s^{\\prime},s),r(s,a,s^{\\prime}),\\gamma>$  \n",
    "**Output** $\\pi^{*}(s),\\quad \\forall s\\in S$\n",
    "\n",
    "Set *V* to arbitrary value function; e.g., $V(s)=0$ for all $s$  \n",
    "\n",
    "**repeat**  \n",
    "    $\\quad\\Delta \\leftarrow 0$  \n",
    "    $\\quad$**for each** $s\\in S$  \n",
    "        $\\quad\\quad$**for each** $a\\in A$  \n",
    "            $\\quad\\quad\\quad Q(s,a)\\leftarrow \\sum_{s^{\\prime}\\in S}P_a(s^{\\prime}|s)[r(s,a,s^{\\prime})+\\gamma V(s^{\\prime})]$  $\\leftarrow$ Bellman equation  \n",
    "        $\\quad\\quad$**end**       \n",
    "        $\\quad\\quad\\Delta \\leftarrow \\max(\\Delta,|\\max_{a\\in A(s)}Q(s,a)-V(s)|)$   \n",
    "        $\\quad\\quad V(s)\\leftarrow \\max_{a\\in A(s)}Q(s,a)$  \n",
    "    $\\quad$**end**     \n",
    "**until** $\\Delta \\leq \\epsilon$\n",
    "\n",
    "\n",
    "**for each** $s\\in S$  \n",
    "    $\\quad\\pi^{*}(s)\\leftarrow\\text{argmax}_a \\sum_{s^{\\prime}\\in S}P_a(s^{\\prime}|s)[r(s,a,s^{\\prime})+\\gamma V(s^{\\prime})]$    \n",
    "**end**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b16a5",
   "metadata": {},
   "source": [
    "The $\\epsilon$ in value iteration is slightly different from in policy iteration, here is pre-defined acceptable error to actual $V^{*}$ because it may need infinite iterations to reach optimal value. So more specificly, the output is the $V$ is the $V$ which is the most close to $V^{*}$ we can generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e790d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n",
      "Final value function:\n",
      "[ 0.       -0.25     -0.3125   -0.328125 -0.25     -0.3125   -0.328125\n",
      " -0.3125   -0.3125   -0.328125 -0.3125   -0.25     -0.328125 -0.3125\n",
      " -0.25      0.      ]\n",
      "Optimal policy:\n",
      "State 1: ['left']\n",
      "State 2: ['left']\n",
      "State 3: ['down', 'left']\n",
      "State 4: ['up']\n",
      "State 5: ['up', 'left']\n",
      "State 6: ['up', 'down', 'left', 'right']\n",
      "State 7: ['down']\n",
      "State 8: ['up']\n",
      "State 9: ['up', 'down', 'left', 'right']\n",
      "State 10: ['down', 'right']\n",
      "State 11: ['down']\n",
      "State 12: ['up', 'right']\n",
      "State 13: ['right']\n",
      "State 14: ['right']\n"
     ]
    }
   ],
   "source": [
    "def value_iteration():\n",
    "    global v, q\n",
    "    print(\"Value Iteration\")\n",
    "    epsilon = 0.001\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for k in range(1, 15):\n",
    "            for j in range(4):\n",
    "                next_state = transition(k, j)\n",
    "                q[k][j] = 0.25 * (-1 + v[next_state])\n",
    "            delta = max(delta, abs(np.max(q[k]) - v[k]))\n",
    "            v[k] = np.max(q[k])\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    \n",
    "    for k in range(1,15):\n",
    "        argmax = np.where(q[k] == np.max(q[k]))[0]\n",
    "        p = np.zeros(4)\n",
    "        p[argmax] = 1/len(argmax)\n",
    "        pi[k] = p\n",
    "    \n",
    "    return pi\n",
    "\n",
    "\n",
    "    \n",
    "_, v, q = initalize()\n",
    "pi = value_iteration()\n",
    "policy = mapping_policy(pi)\n",
    "print(\"Final value function:\")\n",
    "print(v)\n",
    "print(\"Optimal policy:\")\n",
    "for k in range(1, 15):\n",
    "    print(f\"State {k}: {policy[k]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
