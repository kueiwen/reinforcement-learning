{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f60394f8",
   "metadata": {},
   "source": [
    "# **Proximal Policy Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ddb30c",
   "metadata": {},
   "source": [
    "In [policy_gradient](https://github.com/kueiwen/reinforcement-learning/blob/main/policy_gradient.ipynb), we have discussed that Proximal Policy Optimization (PPO) is an algorithm based on policy gradient.\n",
    "\n",
    "PPO is an improved algorithm of TRPO (Trust Region Policy Optimzation), the main difference between TRPO and PPO is constraint function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d09a8",
   "metadata": {},
   "source": [
    "In trust_region_policy_gradient, the surrogate objective function is maximized subject to a constraint on the size of policy update.\n",
    "\n",
    "$$\\max_{\\theta}\\hat{E}_{t}[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\hat{A}_{t}(s_t,a_t)]$$\n",
    "\n",
    "$$\\text{subject to  }\\hat{E}_{t}[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(.|s)||\\pi_{\\theta}(.|s))]\\leq\\delta$$\n",
    "\n",
    "The theory justifying TRPO suggests using a penalty instead of constraint,\n",
    "\n",
    "$$\\max_{\\theta}\\hat{E}_{t}[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\hat{A}_{t}(s_t,a_t)]-\\beta(D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(.|s)||\\pi_{\\theta}(.|s)))$$\n",
    "\n",
    "$\\beta$ s the coefficient of penalty; however, it is hard to find the best $\\beta$ for a reinforcement learning problem to achieve the goal of a first-order algorithm that the monotonic improvement of TRPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe698d0",
   "metadata": {},
   "source": [
    "## **Clipped surrogate objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe166e2",
   "metadata": {},
   "source": [
    "Let $r_t(\\theta)$ denote the probability ratio $r_t(\\theta)=\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}}$ \n",
    "\n",
    "Then TRPO surrogate objective function can be expressed as \n",
    "\n",
    "$$L^{\\text{CPI}}(\\theta)=\\hat{E}_{t}[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\hat{A}_{t}(s_t,a_t)]=\\hat{E}_{t}[r_t(\\theta)\\hat{A}_t]$$\n",
    "\n",
    "*CPI* refers to conservative policy iteration.\n",
    "\n",
    "If $r_t(\\theta)>1$, the action $a_t$ at state $s_t$ is more likely in the current policy than the old policy.     \n",
    "If $r_t(\\theta)$ is between $0$ and $1$, the action $a_t$ at state $s_t$ is less likely in the current policy than the old policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e3b43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Without a constraint, maximization of $L^{\\text{CPI}}$ would lead to an excessively large policy update, so PPO changes to constrain the policy that move $r_t(\\theta)$ away from 1.\n",
    "\n",
    "$$L^{\\text{CLIP}}(\\theta)=\\hat{E}_t[\\min(r_t(\\theta)\\hat{A}_t,\\text{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t)]$$\n",
    "\n",
    "$\\epsilon$ is a hyperparameter, usually set to be 0.2.\n",
    "\n",
    "The first part in *min* is unclipped part $L^{\\text{CPI}}$, the second part, $\\text{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t$, modifies the surrogate objective by clipping the probability ratio within $[1-\\epsilon,1+\\epsilon]$. Lastly, taking the minimum of the clipped and unclipped objective, whcih result in a lower bound on the unclipped objective.\n",
    "\n",
    "Note that the probability ratio $r$ is clipped at $1-\\epsilon$ or $1+\\epsilon$, no matter $\\hat{A}_t$ is positive or negative.\n",
    "\n",
    "<img src=\"img/ppo_clip.png\" width=\"600\"> \n",
    "\n",
    "|  $r_t(\\theta)$   | $A_t$  | Return of $\\min$ | Clipped | Objective | Gradient | \n",
    "|  ----  | ----  | ----  | ----  | ----  | ----  |\n",
    "| $r_t(\\theta)\\in[1-\\epsilon,1+\\epsilon]$  | + | $r_t(\\theta)A_t$ | No | + | Yes | \n",
    "| $r_t(\\theta)\\in[1-\\epsilon,1+\\epsilon]$  | - | $r_t(\\theta)A_t$ | No | - | Yes | \n",
    "| $r_t(\\theta)<1-\\epsilon$  | + | $r_t(\\theta)A_t$ | No | + | Yes | \n",
    "| $r_t(\\theta)<1-\\epsilon$  | - | $(1-\\epsilon)A_t$ | Yes | - | 0 | \n",
    "| $r_t(\\theta)>1+\\epsilon$  | + | $(1+\\epsilon)A_t$ | Yes | + | 0 | \n",
    "| $r_t(\\theta)>1+\\epsilon$  | - | $r_t(\\theta)A_t$ | No | - | Yes | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0d437",
   "metadata": {},
   "source": [
    "If the advantage $A_t$ is positive, which means that the action at that state is better than other actions, so it is encouraged to increase the probability to take that action; in contrast, If the advantage $A_t$ is negative, which means that the action at that state is worse than other actions, so it is disencouraged to increase the probability to take that action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ac234c",
   "metadata": {},
   "source": [
    "## **Adaptive KL panelty coefficient**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a974e6",
   "metadata": {},
   "source": [
    "* Using several epochs of minibatch SGD, optimize the KL-penalized objective\n",
    "\n",
    "$$L^{\\text{KLPEN}}(\\theta)=\\hat{E}_t[\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\hat{A}_{t}(s_t,a_t)]-\\beta(D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(.|s)||\\pi_{\\theta}(.|s)))$$\n",
    "\n",
    "* Compute $d=\\hat{E}_t[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(.|s)||\\pi_{\\theta}(.|s))]$\n",
    "\n",
    "    * If $d<d_{\\text{targ}}/1.5$, $\\beta\\leftarrow\\beta /2$\n",
    "    * If $d>d_{\\text{targ}}\\times 1.5$, $\\beta\\leftarrow\\beta\\times 2$\n",
    "\n",
    "\n",
    "With this scheme, we can see that policy updates is significantly different from $d_{\\text{targ}}$. The parameter $1.5$ and $2$ are chosen heuristically, but the algorithm is not sensitive to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0985289",
   "metadata": {},
   "source": [
    "## **Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74424ea2",
   "metadata": {},
   "source": [
    "***PPO-Clip***\n",
    "\n",
    "---\n",
    "**Input**: initial policy parameter $\\theta_0$, initial value function parameter $\\phi_0$\n",
    "\n",
    "**repeat for k in 0,1,...,L**  \n",
    "    $\\quad$ Collect set of trajectory $D_k={\\tau_i}$ by running policy $\\pi_k=\\pi(\\theta_k)$ in the environment       \n",
    "    $\\quad$ Compute rewards-to-go $\\hat{R}_t$        \n",
    "    $\\quad$ Compute advantage estimates, $\\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\\phi_k}$   \n",
    "    $\\quad$ Update the policy by maximizing the PPO-Clip objective, typically via stochastic gradient ascent with Adam    \n",
    "    $\\quad\\quad\\theta_{k+1}=\\arg\\max_{\\theta}=\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum_{t=0}^T\\min(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\\hat{A}^{\\pi_{\\theta_k}}(s_t,a_t),g(\\epsilon,\\hat{A}^{\\pi_{\\theta_k}}(s_t,a_t)))$      \n",
    "    $\\quad$ Fit value function by regression on mean-square error, typically via some gradient descent algorithm    \n",
    "    $\\quad\\quad\\phi_{k+1}=\\arg\\min_{\\phi}\\frac{1}{|D_k|T}\\sum_{\\tau\\in D_k}\\sum_{t=0}^T(V_{\\phi}(s_t)-\\hat{R}_t)^2$   \n",
    "**end** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c153e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.optimize # For L-BFGS\n",
    "import numpy as np\n",
    "from typing import Tuple, Callable, Dict, Any\n",
    "\n",
    "# Precompute constant\n",
    "LOG_2_PI = np.log(2 * np.pi)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    A Gaussian policy network for continuous action spaces.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Dimension of the state space.\n",
    "        hidden_dim: Dimension of the hidden layers.\n",
    "        output_dim: Dimension of the action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super(Policy, self).__init__()\n",
    "        self.inputLayer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hiddenLayer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.outputLayer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.outputLayer.weight.data.uniform_(-0.003, 0.003)\n",
    "        self.outputLayer.bias.data.uniform_(-0.003, 0.003)\n",
    "\n",
    "        # Learnable log standard deviation by nn.Parameter\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, output_dim))\n",
    "        # Clamping log_std can improve stability\n",
    "        self.log_std_min = -20\n",
    "        self.log_std_max = 2\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass to get action distribution parameters.\n",
    "\n",
    "        Args:\n",
    "            state: Input state tensor.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - action_mean: Mean of the action distribution.\n",
    "            - action_log_std: Log standard deviation of the action distribution.\n",
    "            - action_std: Standard deviation of the action distribution.\n",
    "        \"\"\"\n",
    "        x = torch.tanh(self.inputLayer(x))\n",
    "        x = torch.tanh(self.hiddenLayer(x))\n",
    "        action_mean = self.outputLayer(x)\n",
    "\n",
    "        # Clamp log_std for stability\n",
    "        self.log_std.data.clamp_(self.log_std_min, self.log_std_max)\n",
    "\n",
    "        action_log_std = self.log_std.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_log_std)\n",
    "        return action_mean, action_log_std, action_std\n",
    "    \n",
    "    def get_log_probability_density(self, states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the log probability density of actions under the policy.\n",
    "\n",
    "        Args:\n",
    "            states: State tensor.\n",
    "            actions: Action tensor.\n",
    "\n",
    "        Returns:\n",
    "            Log probability density for each state-action pair.\n",
    "        \"\"\"\n",
    "        action_mean, action_log_std, action_std = self.forward(states)\n",
    "        var = torch.exp(action_log_std).pow(2)\n",
    "        log_prob_per_dim = -0.5 * (((actions - action_mean) / action_std)**2) \\\n",
    "                           - action_log_std \\\n",
    "                           - 0.5 * LOG_2_PI\n",
    "        return log_prob_per_dim\n",
    "    \n",
    "    def get_KL_divergence(self, states: torch.Tensor, actions: torch.Tensor, old_log_prob: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Estimate the KL divergence D_KL(old_policy || current_policy) using samples.\n",
    "        Assumes 'old_log_prob' contains log probabilities from the sampling policy.\n",
    "\n",
    "        Args:\n",
    "            states: State tensor.\n",
    "            actions: Action tensor (sampled from the old policy).\n",
    "            old_log_prob: Log probability of the actions under the old policy.\n",
    "\n",
    "        Returns:\n",
    "            Mean KL divergence estimate.\n",
    "        \"\"\"\n",
    "        current_log_prob = self.get_log_probability_density(states, actions)\n",
    "        kl_div = old_log_prob - current_log_prob\n",
    "        return kl_div.mean()\n",
    "    \n",
    "    def get_action(self, state: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample or get the mean action from the policy.\n",
    "\n",
    "        Args:\n",
    "            state: Input state tensor (should be preprocessed, e.g., unsqueezed).\n",
    "            deterministic: If True, return the mean action. Otherwise, sample.\n",
    "\n",
    "        Returns:\n",
    "            Action tensor.\n",
    "        \"\"\"\n",
    "        with torch.no_grad(): # No need to track gradients for action selection\n",
    "            action_mean, _, action_std = self.forward(state)\n",
    "            if deterministic:\n",
    "                return action_mean\n",
    "            else:\n",
    "                normal = torch.distributions.normal.Normal(action_mean, action_std)\n",
    "                return normal.sample()\n",
    "    \n",
    "\n",
    "class Value(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple MLP value function network.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Dimension of the state space.\n",
    "        hidden_dim: Dimension of the hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super(Value, self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.value_head.weight.data.uniform_(-0.003, 0.003)\n",
    "        self.value_head.bias.data.uniform_(-0.003, 0.003)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(x).unsqueeze(0)\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = torch.tanh(self.hidden2(x))\n",
    "        value = self.value_head(x)\n",
    "        return value\n",
    "    \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic architecture combining policy and value networks.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Dimension of the state space.\n",
    "        hidden_dim: Dimension of the hidden layers.\n",
    "        output_dim: Dimension of the action space.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = Policy(input_dim, hidden_dim, output_dim)\n",
    "        self.critic = Value(input_dim, hidden_dim)\n",
    "\n",
    "    def set_action_std(self, action_std: float):\n",
    "        \"\"\"\n",
    "        Set the action standard deviation for the policy.\n",
    "\n",
    "        Args:\n",
    "            action_std: Standard deviation for the action distribution.\n",
    "        \"\"\"\n",
    "        self.actor.log_std.data.fill_(np.log(action_std))\n",
    "\n",
    "    def get_action(self, state: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get action from the policy.\n",
    "\n",
    "        Args:\n",
    "            state: Input state tensor.\n",
    "            deterministic: If True, return the mean action. Otherwise, sample.\n",
    "\n",
    "        Returns:\n",
    "            Action tensor.\n",
    "        \"\"\"\n",
    "        action = self.actor.get_action(state, deterministic)\n",
    "        log_prob = self.actor.get_log_probability_density(state, action).sum(dim=1, keepdim=True)\n",
    "        state_val = self.critic(state)\n",
    "        return action.detach(), log_prob.detach(), state_val.detach()\n",
    "    \n",
    "    def evaluate(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Evaluate the policy and value for a given state-action pair.\n",
    "\n",
    "        Args:\n",
    "            state: Input state tensor.\n",
    "            action: Action tensor.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "            - log_prob: Log probability of the action under the policy.\n",
    "            - state_val: Value of the state from the critic.\n",
    "            - entropy: Entropy of the action distribution.\n",
    "        \"\"\"\n",
    "        log_prob = self.actor.get_log_probability_density(state, action)\n",
    "        entropy = -(log_prob * torch.exp(log_prob)).sum(dim=1, keepdim=True)\n",
    "        state_val = self.critic(state)\n",
    "        return log_prob.sum(dim=1, keepdim=True), state_val, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c5eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "gamma = 0.99 # Discount factor\n",
    "tau = 0.95 # GAE lambda parameter\n",
    "epsilon = 0.2 # PPO clipping parameter\n",
    "policy_net = Policy(num_inputs, hidden_dim, num_actions)\n",
    "value_net = Value(num_inputs, hidden_dim, num_actions)\n",
    "n_episodes = 1000 # Example total episodes\n",
    "batch_size = 4000  # Target number of steps per policy update batch\n",
    "max_episode_steps = 1000 # Max steps per episode\n",
    "log_interval = 100 # How often to print logs\n",
    "rewards = []\n",
    "\n",
    "def update_policy(batch: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Updates the policy and value networks using PPO.\n",
    "\n",
    "    Args:\n",
    "        batch: A dictionary containing 'states', 'actions', 'rewards', 'mask'.\n",
    "               Assumes data corresponds to a single trajectory or episode.\n",
    "    \"\"\"\n",
    "    # --- 1. Data Preparation ---\n",
    "    # Consider device placement (.to(device)) if using GPU\n",
    "    # Using torch.as_tensor is generally safer than torch.FloatTensor\n",
    "    # Avoid squeeze(0) unless batch structure guarantees dim 0 is size 1.\n",
    "    # Assume batch contains data for N steps: [N, state_dim], [N, action_dim], etc.\n",
    "    try:\n",
    "        states = torch.as_tensor(batch[\"states\"], dtype=torch.float32)\n",
    "        actions = torch.as_tensor(batch[\"actions\"], dtype=torch.float32)\n",
    "        rewards = torch.as_tensor(batch[\"rewards\"], dtype=torch.float32)\n",
    "        # Ensure masks are treated as floats for multiplication\n",
    "        masks = torch.as_tensor(batch[\"mask\"], dtype=torch.float32)\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Batch dictionary missing key: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch tensors: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Validate shapes - assuming [N, dim] format after potential loading squeeze\n",
    "    if states.dim() == 1: states = states.unsqueeze(0) # Handle single step case\n",
    "    if actions.dim() == 1: actions = actions.unsqueeze(0)\n",
    "    if rewards.dim() == 1: rewards = rewards.unsqueeze(0)\n",
    "    if masks.dim() == 1: masks = masks.unsqueeze(0)\n",
    "\n",
    "    # Ensure rewards and masks have a trailing dimension for broadcasting if needed\n",
    "    if rewards.dim() == 1: rewards = rewards.unsqueeze(-1) # Shape [N, 1]\n",
    "    if masks.dim() == 1: masks = masks.unsqueeze(-1)     # Shape [N, 1]\n",
    "    if actions.dim() == 1: actions = actions.unsqueeze(-1) # Shape [N, 1] if action_dim is 1\n",
    "\n",
    "    \n",
    "    # --- 2. Value Function Estimation ---\n",
    "    with torch.no_grad(): # No gradients needed for calculating targets\n",
    "        # Typo corrected: squeeeze -> squeeze\n",
    "        # Use squeeze(-1) if value_net outputs [N, 1], or just ensure output is [N]\n",
    "        values = value_net(states).squeeze(0) # Assuming output [N, 1] -> [N]\n",
    "\n",
    "\n",
    "    # --- 3. GAE and Returns Calculation ---\n",
    "    num_steps = rewards.size(0)\n",
    "    returns = torch.zeros_like(rewards)     # Use zeros_like for correct shape/device/dtype\n",
    "    deltas = torch.zeros_like(rewards)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "\n",
    "    prev_return = 0.0\n",
    "    prev_value = 0.0\n",
    "    prev_advantage = 0.0\n",
    "    for i in reversed(range(num_steps)):\n",
    "    # Ensure rewards[i], masks[i], values[i] are scalars or broadcastable\n",
    "        # Using .item() might be safer if shapes are guaranteed [1], but indexing should work for [N]\n",
    "        current_reward = rewards[i]\n",
    "        current_mask = masks[i]\n",
    "        current_value = values[i] # From value_net(states) calculated earlier\n",
    "\n",
    "        # Calculate return G(t) = r_t + gamma * G(t+1) * mask\n",
    "        returns[i] = current_reward + gamma * prev_return * current_mask\n",
    "\n",
    "        # Calculate TD error (delta) = r_t + gamma * V(s_{t+1}) * mask - V(s_t)\n",
    "        # Note: prev_value holds V(s_{t+1}) from the previous iteration\n",
    "        deltas[i] = current_reward + gamma * prev_value * current_mask - current_value\n",
    "\n",
    "        # Calculate GAE advantage A(t) = delta_t + gamma * tau * A(t+1) * mask\n",
    "        advantages[i] = deltas[i] + gamma * tau * prev_advantage * current_mask\n",
    "\n",
    "        # Update values for the next iteration (t-1)\n",
    "        # Use detach() instead of .data if accessing tensors that might have history\n",
    "        prev_return = returns[i].item() # Use .item() for scalar python number\n",
    "        prev_value = current_value.item() # V(s_t) becomes V(s_{t+1}) for next step\n",
    "        prev_advantage = advantages[i].item()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "for i_episode in range(n_episodes):\n",
    "    # Data storage for the current batch (will collect multiple episodes)\n",
    "    batch_states = []\n",
    "    batch_actions = []\n",
    "    batch_rewards = []\n",
    "    batch_masks = [] # Represents (1 - done)\n",
    "\n",
    "    steps_in_batch = 0\n",
    "    episodes_in_batch = 0\n",
    "    total_reward_in_batch = 0.0\n",
    "\n",
    "    # Collect experience until batch_size is reached\n",
    "    while steps_in_batch < batch_size:\n",
    "        state = env.reset()\n",
    "        # Ensure state is in the format expected by policy_net (e.g., numpy array)\n",
    "        # If policy_net expects a tensor, convert here:\n",
    "        # state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "\n",
    "        # Temporary storage for the current episode's trajectory\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        episode_masks = []\n",
    "\n",
    "        for t in range(max_episode_steps):\n",
    "            # 1. Get Action\n",
    "            # Ensure state format matches policy_net.get_action input requirement\n",
    "            # Assuming get_action returns a tensor\n",
    "            state_tensor = torch.from_numpy(state.reshape(-1)).float().unsqueeze(0)\n",
    "            action_tensor = policy_net.get_action(state_tensor)\n",
    "            # Convert action to numpy for the environment step if needed\n",
    "            action_numpy = action_tensor.detach().cpu().numpy() # Adjust based on env requirements\n",
    "\n",
    "            # 2. Step Environment\n",
    "            # Ensure env.step returns consistent types (usually numpy for state/reward)\n",
    "            next_state, reward, done, _ = env.step(action_numpy)\n",
    "\n",
    "            # 3. Store Transition Data (using consistent types, e.g., numpy for states)\n",
    "            episode_states.append(state.reshape(-1)) # Store original state (numpy)\n",
    "            episode_actions.append(action_tensor) # Store action tensor\n",
    "            episode_rewards.append(reward) # Store reward (float/numpy)\n",
    "            episode_masks.append(1.0 - float(done)) # Store mask (float)\n",
    "\n",
    "            state = next_state # Update state for next iteration\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # End of episode: Append episode data to the main batch lists\n",
    "        batch_states.extend(episode_states)\n",
    "        batch_actions.extend(episode_actions)\n",
    "        batch_rewards.extend(episode_rewards)\n",
    "        batch_masks.extend(episode_masks)\n",
    "\n",
    "        # Update batch counters\n",
    "        steps_in_batch += episode_steps\n",
    "        total_reward_in_batch += episode_reward\n",
    "        episodes_in_batch += 1\n",
    "\n",
    "        # Store the reward of the *last completed* episode for logging\n",
    "        last_episode_reward = episode_reward\n",
    "\n",
    "    # --- Batch Finalization and Policy Update ---\n",
    "    # Calculate average reward per episode in this batch\n",
    "    avg_reward_per_episode = total_reward_in_batch / episodes_in_batch if episodes_in_batch > 0 else 0.0\n",
    "    total_steps_processed += steps_in_batch\n",
    "\n",
    "    # Prepare batch dictionary for update_policy\n",
    "    # Convert lists of data points into single tensors\n",
    "    # Ensure correct dtypes and device placement (.to(device)) if using GPU\n",
    "    update_batch = {\n",
    "        \"states\": torch.tensor(np.asarray(batch_states), dtype=torch.float32),\n",
    "        \"actions\": torch.stack(batch_actions), # Stack list of action tensors\n",
    "        \"rewards\": torch.tensor(batch_rewards, dtype=torch.float32).unsqueeze(1), # Add dim for [N, 1]\n",
    "        \"mask\": torch.tensor(batch_masks, dtype=torch.float32).unsqueeze(1)      # Add dim for [N, 1]\n",
    "        # Note: \"next_states\" is often not needed directly by GAE/TRPO update,\n",
    "        # but if it were, you'd collect and tensorize it similarly.\n",
    "    }\n",
    "\n",
    "    # Call the policy update function\n",
    "    update_policy(update_batch) # Pass the correctly formatted batch\n",
    "    rewards.append(avg_reward_per_episode[0])\n",
    "    # --- Logging ---\n",
    "    if i_episode % log_interval == 0:\n",
    "        print(f'Episode {i_episode}\\tSteps Collected: {steps_in_batch}\\t'\n",
    "              f'Last Ep Reward: {last_episode_reward[0]:.2f}\\t'\n",
    "              f'Avg Batch Ep Reward: {avg_reward_per_episode[0]:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
